<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Send a prompt to a LLM provider — send_prompt • tidyprompt</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Send a prompt to a LLM provider — send_prompt"><meta name="description" content="This function is responsible for sending prompts to a LLM provider for evaluation.
The function will interact with the LLM provider until a successful response
is received or the maximum number of interactions is reached. The function will
apply extraction and validation functions to the LLM response, as specified
in the prompt wraps (see prompt_wrap()). If the maximum number of interactions"><meta property="og:description" content="This function is responsible for sending prompts to a LLM provider for evaluation.
The function will interact with the LLM provider until a successful response
is received or the maximum number of interactions is reached. The function will
apply extraction and validation functions to the LLM response, as specified
in the prompt wraps (see prompt_wrap()). If the maximum number of interactions"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">tidyprompt</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.1.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/creating_prompt_wraps.html">Creating prompt wraps</a></li>
    <li><a class="dropdown-item" href="../articles/getting_started.html">Getting started</a></li>
    <li><a class="dropdown-item" href="../articles/sentiment_analysis.html">Sentiment analysis in R with a LLM and 'tidyprompt'</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/tjarkvandemerwe/tidyprompt/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Send a prompt to a LLM provider</h1>
      <small class="dont-index">Source: <a href="https://github.com/tjarkvandemerwe/tidyprompt/blob/main/R/send_prompt.R" class="external-link"><code>R/send_prompt.R</code></a></small>
      <div class="d-none name"><code>send_prompt.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>This function is responsible for sending prompts to a LLM provider for evaluation.
The function will interact with the LLM provider until a successful response
is received or the maximum number of interactions is reached. The function will
apply extraction and validation functions to the LLM response, as specified
in the prompt wraps (see <code><a href="prompt_wrap.html">prompt_wrap()</a></code>). If the maximum number of interactions</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">send_prompt</span><span class="op">(</span></span>
<span>  <span class="va">prompt</span>,</span>
<span>  llm_provider <span class="op">=</span> <span class="fu"><a href="llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  max_interactions <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  clean_chat_history <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  stream <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  return_mode <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"only_response"</span>, <span class="st">"full"</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-prompt">prompt<a class="anchor" aria-label="anchor" href="#arg-prompt"></a></dt>
<dd><p>A string or a <a href="tidyprompt-class.html">tidyprompt</a> object</p></dd>


<dt id="arg-llm-provider">llm_provider<a class="anchor" aria-label="anchor" href="#arg-llm-provider"></a></dt>
<dd><p><a href="llm_provider-class.html">llm_provider</a> object
(default is <code><a href="llm_provider_ollama.html">llm_provider_ollama()</a></code>).
This object and its settings will be used to evaluate the prompt.
Note that the 'verbose' and 'stream' settings in the LLM provider will be
overruled by the 'verbose' and 'stream' arguments in this function
when those are not NULL.
Furthermore, advanced <a href="tidyprompt-class.html">tidyprompt</a> objects may carry '$parameter_fn'
functions which can set parameters in the llm_provider object
(see <code><a href="prompt_wrap.html">prompt_wrap()</a></code> and <a href="llm_provider-class.html">llm_provider</a> for more ).</p></dd>


<dt id="arg-max-interactions">max_interactions<a class="anchor" aria-label="anchor" href="#arg-max-interactions"></a></dt>
<dd><p>Maximum number of interactions allowed with the
LLM provider. Default is 10. If the maximum number of interactions is reached
without a successful response, 'NULL' is returned as the response (see return
value). The first interaction is the initial chat completion</p></dd>


<dt id="arg-clean-chat-history">clean_chat_history<a class="anchor" aria-label="anchor" href="#arg-clean-chat-history"></a></dt>
<dd><p>If the chat history should be cleaned after each
interaction. Cleaning the chat history means that only the
first and last message from the user, the last message from the assistant,
all messages from the system, and all tool results are kept in a 'clean'
chat history. This clean chat history is used when requesting a new chat completion.
(i.e., if a LLM repeatedly fails to provide a correct response, only its last failed response
will included in the context window). This may increase the LLM performance
on the next interaction</p></dd>


<dt id="arg-verbose">verbose<a class="anchor" aria-label="anchor" href="#arg-verbose"></a></dt>
<dd><p>If the interaction with the LLM provider should be printed
to the console. This will overrule the 'verbose' setting in the LLM provider</p></dd>


<dt id="arg-stream">stream<a class="anchor" aria-label="anchor" href="#arg-stream"></a></dt>
<dd><p>If the interaction with the LLM provider should be streamed.
This setting will only be used if the LLM provider already has a
'stream' parameter (which indicates there is support for streaming). Note
that when 'verbose' is set to FALSE, the 'stream' setting will be ignored</p></dd>


<dt id="arg-return-mode">return_mode<a class="anchor" aria-label="anchor" href="#arg-return-mode"></a></dt>
<dd><p>One of 'full' or 'only_response'. See return value</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>

<ul><li><p>If return mode 'only_response', the function will return only the LLM response
after extraction and validation functions have been applied (NULL is returned
when unsuccessful after the maximum number of interactions).</p></li>
<li><p>If return mode 'full', the function will return a list with the following elements:</p><ul><li><p>'response' (the LLM response after extraction and validation functions have been applied;
NULL is returned when unsuccessful after the maximum number of interactions),</p></li>
<li><p>'interactions' (the number of interactions with the LLM provider),</p></li>
<li><p>'chat_history' (a dataframe with the full chat history which led to the final response),</p></li>
<li><p>'chat_history_clean' (a dataframe with the cleaned chat history which led to
the final response; here, only the first and last message from the user, the
last message from the assistant, and all messages from the system are kept),</p></li>
<li><p>'start_time' (the time when the function was called),</p></li>
<li><p>'end_time' (the time when the function ended),</p></li>
<li><p>'duration_seconds' (the duration of the function in seconds), and</p></li>
<li><p>'http_list' (a list with all HTTP responses made during the interactions;
as returned by <code>llm_provider$complete_chat()</code>).</p></li>
</ul></li>
</ul></div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p><a href="tidyprompt-class.html">tidyprompt</a>, <code><a href="prompt_wrap.html">prompt_wrap()</a></code>, <a href="llm_provider-class.html">llm_provider</a>, <code><a href="llm_provider_ollama.html">llm_provider_ollama()</a></code>,
<code><a href="llm_provider_openai.html">llm_provider_openai()</a></code></p>
<p>Other prompt_evaluation:
<code><a href="llm_break.html">llm_break</a>()</code>,
<code><a href="llm_feedback.html">llm_feedback</a>()</code></p></div>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span>  <span class="st">"Hi!"</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">send_prompt</span><span class="op">(</span><span class="fu"><a href="llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Sending request to LLM provider (llama3.1:8b): ---</span></span></span>
<span class="r-in"><span>  <span class="co">#   Hi!</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Receiving response from LLM provider: ---</span></span></span>
<span class="r-in"><span>  <span class="co">#   It's nice to meet you. Is there something I can help you with, or would you like to chat?</span></span></span>
<span class="r-in"><span>  <span class="co"># [1] "It's nice to meet you. Is there something I can help you with, or would you like to chat?"</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span>  <span class="st">"Hi!"</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">send_prompt</span><span class="op">(</span><span class="fu"><a href="llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span>, return_mode <span class="op">=</span> <span class="st">"full"</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Sending request to LLM provider (llama3.1:8b): ---</span></span></span>
<span class="r-in"><span>  <span class="co">#   Hi!</span></span></span>
<span class="r-in"><span>  <span class="co"># --- Receiving response from LLM provider: ---</span></span></span>
<span class="r-in"><span>  <span class="co">#   It's nice to meet you. Is there something I can help you with, or would you like to chat?</span></span></span>
<span class="r-in"><span>  <span class="co"># $response</span></span></span>
<span class="r-in"><span>  <span class="co"># [1] "It's nice to meet you. Is there something I can help you with, or would you like to chat?"</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># $chat_history</span></span></span>
<span class="r-in"><span>  <span class="co"># ...</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># $chat_history_clean</span></span></span>
<span class="r-in"><span>  <span class="co"># ...</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># $start_time</span></span></span>
<span class="r-in"><span>  <span class="co"># [1] "2024-11-18 15:43:12 CET"</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># $end_time</span></span></span>
<span class="r-in"><span>  <span class="co"># [1] "2024-11-18 15:43:13 CET"</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># $duration_seconds</span></span></span>
<span class="r-in"><span>  <span class="co"># [1] 1.13276</span></span></span>
<span class="r-in"><span>  <span class="co">#</span></span></span>
<span class="r-in"><span>  <span class="co"># $http_list</span></span></span>
<span class="r-in"><span>  <span class="co"># $http_list[[1]]</span></span></span>
<span class="r-in"><span>  <span class="co"># Response [http://localhost:11434/api/chat]</span></span></span>
<span class="r-in"><span>  <span class="co">#   Date: 2024-11-18 14:43</span></span></span>
<span class="r-in"><span>  <span class="co">#   Status: 200</span></span></span>
<span class="r-in"><span>  <span class="co">#   Content-Type: application/x-ndjson</span></span></span>
<span class="r-in"><span>  <span class="co"># &lt;EMPTY BODY&gt;</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span>  <span class="st">"Hi!"</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="add_text.html">add_text</a></span><span class="op">(</span><span class="st">"What is 5 + 5?"</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="answer_as_integer.html">answer_as_integer</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>    <span class="fu">send_prompt</span><span class="op">(</span><span class="fu"><a href="llm_provider_ollama.html">llm_provider_ollama</a></span><span class="op">(</span><span class="op">)</span>, verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="co"># [1] 10</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Luka Koning, Tjark Van de Merwe, Kennispunt Twente.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>

