[{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/creating_prompt_wraps.html","id":"creating-prompt-wraps","dir":"Articles","previous_headings":"","what":"Creating prompt wraps","title":"Creating prompt wraps","text":"Using prompt_wrap(), can create prompt wraps. input prompt_wrap() wrap may string tidyprompt object. pass string, automatically turned tidyprompt object. hood, tidyprompt object just list base prompt (string) series prompt wraps. prompt_wrap() adds new prompt wrap list prompt wraps. prompt wrap list modification function, extraction function, /validation function (least one functions must present). modification function alters prompt text, extraction function applies transformation LLM’s response, validation function checks (transformed) LLM’s response valid. extraction validation functions can return feedback LLM, using llm_feedback(). extraction validation function returns , message sent back LLM, LLM can retry answering prompt according feedback. Feedback messages may reiteration instruction specific error message occured extraction validation. extractions validations applied without resulting feedback, LLM’s response (transformations extraction functions) returned. (send_prompt() responsible executing process.) simple example prompt wrap, just adds text base prompt: Shorter notation : Often times, may preferred make function takes prompt returns wrapped prompt: Take look source code answer_as_boolean(), also uses extraction: Take look source code , instance, answer_as_integer(), answer_by_chain_of_thought(), answer_using_tools() advanced examples prompt wraps.","code":"prompt <- \"Hi there!\" |>   prompt_wrap(     modify_fn = function(base_prompt) {       paste(base_prompt, \"How are you?\", sep = \"\\n\\n\")     }   ) prompt <- \"Hi there!\" |>   prompt_wrap(\\(x) paste(x, \"How are you?\", sep = \"\\n\\n\")) my_prompt_wrap <- function(prompt) {   modify_fn <- function(base_prompt) {     paste(base_prompt, \"How are you?\", sep = \"\\n\\n\")   }    prompt_wrap(prompt, modify_fn) } prompt <- \"Hi there!\" |>   my_prompt_wrap() answer_as_boolean <- function(     prompt,     true_definition = NULL,     false_definition = NULL,     add_instruction_to_prompt = TRUE ) {   instruction <- \"You must answer with only TRUE or FALSE (use no other characters).\"   if (!is.null(true_definition))     instruction <- paste(instruction, glue::glue(\"TRUE means: {true_definition}.\"))   if (!is.null(false_definition))     instruction <- paste(instruction, glue::glue(\"FALSE means: {false_definition}.\"))    modify_fn <- function(original_prompt_text) {     if (!add_instruction_to_prompt) {       return(original_prompt_text)     }      glue::glue(\"{original_prompt_text}\\n\\n{instruction}\")   }    extraction_fn <- function(x) {     normalized <- tolower(trimws(x))     if (normalized %in% c(\"true\", \"false\")) {       return(as.logical(normalized))     }     return(llm_feedback(instruction))   }    prompt_wrap(prompt, modify_fn, extraction_fn) }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/creating_prompt_wraps.html","id":"breaking-out-of-the-evaluation-loop","dir":"Articles","previous_headings":"Creating prompt wraps","what":"Breaking out of the evaluation loop","title":"Creating prompt wraps","text":"cases, may want exit extraction validation process early. instance, LLM may indicate unable answer prompt. cases, can extraction validation function return llm_break(). cause evaluation loop break, forwarding return statement send_prompt(). See quit_if() example .","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/creating_prompt_wraps.html","id":"extraction-versus-validation-functions","dir":"Articles","previous_headings":"Creating prompt wraps","what":"Extraction versus validation functions","title":"Creating prompt wraps","text":"extraction validation functions can return llm_break() llm_feedback(). difference extraction validation functions extraction may transform LLM response pass next extraction /validation functions, validation function checks LLM response passes logical test (without altering response). Thus, wish, can perform validations extraction function.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/creating_prompt_wraps.html","id":"prompt-wrap-types-and-order-of-application","dir":"Articles","previous_headings":"Creating prompt wraps","what":"Prompt wrap types and order of application","title":"Creating prompt wraps","text":"constructing prompt text evaluating prompt, prompt wraps applied prompt wrap prompt wrap (e.g., first extraction validation functions one wrap, ). order prompt wraps applied important. Currently, four types prompt wraps distinguished: ‘unspecified’, ‘break’, ‘mode’, ‘tool’. constructing prompt text, prompt wraps applied order types. Prompt wraps automatically reordered necesarry (keeping intact order prompt wraps type). evaluating prompt, prompt wraps applied reverse order types (.e., first ‘tool’, ‘mode’, ‘break’, finally ‘unspecified’). ‘tool’ prompt wraps may return value used final answer, ‘mode’ prompt wraps alter LLM forms final answer, ‘break’ prompt wraps quit evaluation early based specific final answer, ‘unspecified’ prompt wraps general type prompt wraps force final answer specific format.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/creating_prompt_wraps.html","id":"configuring-a-llm-provider-with-a-prompt-wrap","dir":"Articles","previous_headings":"Creating prompt wraps","what":"Configuring a LLM provider with a prompt wrap","title":"Creating prompt wraps","text":"Advanced prompt wraps may wish configure certain settings LLM provider. instance, answer_as_json() configures certain parameters LLM provider, based LLM provider used (Ollama OpenAI different API parameters available JSON output). done defining parameter_fn within prompt wrap; parameter_fn function takes LLM provider input returns list parameters, set parameters LLM provider sending prompt. See prompt_wrap() documentation answer_as_json() example.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/creating_prompt_wraps.html","id":"configuring-a-prompt-wrap-based-on-the-llm-provider-or-http-responses","dir":"Articles","previous_headings":"Creating prompt wraps","what":"Configuring a prompt wrap based on the LLM provider or HTTP responses","title":"Creating prompt wraps","text":"modify_fn, extraction_fn, validation_fn may take LLM provider second argument http_list (list HTTP responses made send_prompt()) third argument. allows advanced configuration prompt extraction validation logic based LLM provider data available HTTP responses.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/creating_prompt_wraps.html","id":"configuring-a-prompt-wrap-based-on-other-prompt-wraps","dir":"Articles","previous_headings":"Creating prompt wraps","what":"Configuring a prompt wrap based on other prompt wraps","title":"Creating prompt wraps","text":"modify_fn, extraction_fn, validation_fn access self object, represents tidyprompt-class object part . allows advanced configuration prompt extraction validation logic based prompt wraps. Inside functions, simply use self$ access tidyprompt-class object.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/creating_prompt_wraps.html","id":"handler-functions","dir":"Articles","previous_headings":"Creating prompt wraps","what":"Handler functions","title":"Creating prompt wraps","text":"Prompt wraps may also include ‘handler functions’. functions called upon every received chat completion. allows advanced processing LLM’s responses, logging, tracking tokens, custom processing. See prompt_wrap() documentation information; see source code answer_using_tools() example handler function.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/getting_started.html","id":"setup-an-llm-provider","dir":"Articles","previous_headings":"","what":"Setup an LLM provider","title":"Getting started","text":"‘tidyprompt’ can used LLM provider capable completing chat. moment, ‘tidyprompt’ includes pre-built functions connect various LLM providers, Ollama, OpenAI, OpenRouter, Mistral, Groq, XAI (Grok), Google Gemini. llm_provider-class, can easily write hook LLM provider. make API calls using ‘httr2’ package use another R package already hook LLM provider want use. API choice follows structure OpenAI API, can call llm_provider_openai() change relevant parameters (like URL API key).","code":"# Ollama running on local PC ollama <- llm_provider_ollama(   parameters = list(model = \"llama3.1:8b\"), )  # OpenAI API openai <- llm_provider_openai(   parameters = list(model = \"gpt-4o-mini\") )  # Various providers via OpenRouter (e.g., Anthropic) openrouter <- llm_provider_openrouter(   parameters = list(model = \"anthropic/claude-3.5-sonnet\") )  # ... functions also included for Mistral, Groq, XAI (Grok), and Google Gemini  # ... or easily create your own hook for any other LLM provider; #   see ?llm_provider-class for more information; also take a look at the source code of #   llm_provider_ollama() and llm_provider_openai(). For APIs that follow the structure #   of the OpenAI API for chat completion, you can use llm_provider_openai()"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/getting_started.html","id":"basic-prompting","dir":"Articles","previous_headings":"","what":"Basic prompting","title":"Getting started","text":"simple string serves base prompt. adding prompt wraps, can influence various aspects LLM handles prompt, verifying output structured valid (including retries feedback LLM ). add_text() simple example prompt wrap. simply adds text end base prompt. can also construct final prompt text, without sending LLM provider.","code":"\"Hi there!\" |>     send_prompt(ollama) #> --- Sending request to LLM provider (llama3.1:8b): --- #> Hi there! #> --- Receiving response from LLM provider: --- #> How's your day going so far? Is there something I can help you with or would you like to chat? #> [1] \"How's your day going so far? Is there something I can help you with or would you like to chat?\" \"Hi there!\" |>     add_text(\"What is a large language model? Explain in 10 words.\") |>     send_prompt(ollama) #> --- Sending request to LLM provider (llama3.1:8b): --- #> Hi there! #>  #> What is a large language model? Explain in 10 words. #> --- Receiving response from LLM provider: --- #> Advanced computer program trained on vast amounts of written data. #> [1] \"Advanced computer program trained on vast amounts of written data.\" \"Hi there!\" |>     add_text(\"What is a large language model? Explain in 10 words.\") #> <tidyprompt> #> The base prompt is modified by a prompt wrap, resulting in: #> > Hi there! #> >  #> > What is a large language model? Explain in 10 words.  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #> Use 'get_prompt_wraps(x)' to show the prompt wraps."},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/getting_started.html","id":"prompt-wraps","dir":"Articles","previous_headings":"","what":"Prompt wraps","title":"Getting started","text":"package contains three main families pre-built prompt wraps affect LLM handles prompt: answer_as: specify format output (e.g., integer, list, json) answer_by: specify reasoning mode reach answer (e.g., chain--thought, ReAct) answer_using: give LLM tools reach answer (e.g., R functions, R code, SQL) , show examples type prompt wrap.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/getting_started.html","id":"answer_as-retrieving-output-in-a-specific-format","dir":"Articles","previous_headings":"Prompt wraps","what":"answer_as: Retrieving output in a specific format","title":"Getting started","text":"Using prompt wraps, can force LLM return output specific format. can also extract output turn character another data type. instance, answer_as_integer() adds prompt wrap forces LLM reply integer. achieve , prompt wrap add text base prompt, asking LLM reply integer. However, prompt wrap : also attempt extract validate integer LLM’s response. extraction validation fails, feedback sent back LLM, LLM can retry answering prompt. extraction function turns original character response numeric value, final output send_prompt() also numeric type. example prompt initially fail, succeed llm_feedback() retry. ‘tidyprompt’ offers various ‘answer_as’ functions, answer_as_boolean(), answer_as_regex_match(), answer_as_named_list(), answer_as_text() answer_as_json().","code":"\"What is 2 + 2?\" |>     answer_as_integer() |>     send_prompt(ollama) #> --- Sending request to LLM provider (llama3.1:8b): --- #> What is 2 + 2? #>  #> You must answer with only an integer (use no other characters). #> --- Receiving response from LLM provider: --- #> 4 #> [1] 4 \"What is 2 + 2?\" |>     add_text(\"Please write out your reply in words, use no numbers.\") |>     answer_as_integer(add_instruction_to_prompt = FALSE) |>     send_prompt(ollama) #> --- Sending request to LLM provider (llama3.1:8b): --- #> What is 2 + 2? #>  #> Please write out your reply in words, use no numbers. #> --- Receiving response from LLM provider: --- #> Four. #> --- Sending request to LLM provider (llama3.1:8b): --- #> You must answer with only an integer (use no other characters). #> --- Receiving response from LLM provider: --- #> 4 #> [1] 4"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/getting_started.html","id":"json-output","dir":"Articles","previous_headings":"Prompt wraps > answer_as: Retrieving output in a specific format","what":"JSON output","title":"Getting started","text":"answer_as_json() may especially powerful LLM provider model native support returning JSON objects adhering JSON schemas (e.g., OpenAI, Ollama). Text-based handling however always possible, also providers natively support functions. means can always switch providers ensuring results correct format. also noted native JSON enforcement may also restrict model much, always good test see works best use case. experience, text-based handling often flexible robust, especially combining multiple prompt wraps. Note also JSON schemas may enforce qualities looking output, may still need add additional prompt wraps ensure output desired.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/getting_started.html","id":"answer_by-adding-a-reasoning-mode-to-the-llm","dir":"Articles","previous_headings":"Prompt wraps","what":"answer_by: Adding a reasoning mode to the LLM","title":"Getting started","text":"Prompt wraps may also used add reasoning mode LLM. hypothesized improve LLM’s performance complex tasks. instance, answer_by_chain_of_thought() add chain thought reasoning mode prompt evaluation LLM. function wraps base prompt text within request LLM reason step step, asking provide final answer within ‘FINISH[]’. extraction function ensures final answer returned.","code":"\"What is 2 + 2?\" |>     answer_by_chain_of_thought() |>     answer_as_integer() |>     send_prompt(ollama) #> --- Sending request to LLM provider (llama3.1:8b): --- #> You are given a user's prompt. #> To answer the user's prompt, you need to think step by step to arrive at a final answer. #>  #> ----- START OF USER'S PROMPT ----- #> What is 2 + 2? #>  #> You must answer with only an integer (use no other characters). #> ----- END OF USER'S PROMPT ----- #>  #> What are the steps you would take to answer the user's prompt? #> Describe your thought process in the following format: #>   >> step 1: <step 1 description> #>   >> step 2: <step 2 description> #>   (etc.) #>  #> When you are done, you must type: #>   FINISH[<put here your final answer to the user's prompt>] #>  #> Make sure your final answer follows the logical conclusion of your thought process. #> --- Receiving response from LLM provider: --- #> Here are my thought steps: #>  #> >> step 1: Understand the question - The prompt asks for the result of a simple arithmetic operation, specifically adding 2 and 2. #>  #> >> step 2: Recall basic math facts - I recall that the sum of two identical numbers is equal to twice the value of each number. In this case, both numbers are 2. #>  #> >> step 3: Apply the math fact - Using the knowledge from step 2, I calculate the result by multiplying 2 (the number being added) by 2 (the other number), which gives 4. #>  #> >> step 4: Confirm the answer - Before providing a final response, I confirm that my calculation is correct. Adding 2 and 2 indeed equals 4. #>  #> FINISH[4] #> [1] 4"},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/getting_started.html","id":"tools-function-calling","dir":"Articles","previous_headings":"Prompt wraps > answer_using: Have the LLM work with tools and code","what":"Tools (function-calling)","title":"Getting started","text":"answer_using_tools(), can enable LLM call R functions. enables LLM autonomously retrieve additional information take actions. answer_using_tools() automatically extracts documentation available base R functions functions packages. Types inferred default arguments function. want define custom function /override default documentation, can use tools_add_docs(). See example usage documentation answer_using_tools(). answer_using_tools() supports text-based function calling native function calling (via API parameters, currently implemented OpenAI Ollama API structures).","code":"\"What are the files in my current directory?\" |>     answer_using_tools(list.files) |>     send_prompt(ollama) #> ! `answer_using_tools()`, `tools_docs_to_text()`: #> * Argument 'pattern' has an unknown type. Defaulting to 'string' #> --- Sending request to LLM provider (llama3.1:8b): --- #> What are the files in my current directory? #>  #> If you need more information, you can call functions to help you. #>  #> To call a function, output a JSON object with the following format: #>   { #>     \"function\": \"<function name>\", #>     \"arguments\": { #>       \"<argument_name>\": <argument_value>, #>       # ... #>     } #>   } #>   (Note: you may not provide function calls as function arguments.) #>  #> The following functions are available: #>  #>   function name: list.files #>   description: List the Files in a Directory/Folder: These functions produce a character vector of the names of files #> or directories in the named directory. #>   arguments: #>     - path: a character vector of full path names; the default corresponds to the working directory, 'getwd()'. (...) [Type: string] #>     - pattern: an optional regular expression.  Only file names which match the regular expression will be returned. [Type: unknown] #>     - all.files: a logical value.  If 'FALSE', only the names of visible files are returned (following Unix-style visibility, that is files whose name does not start with a dot).  If 'TRUE', all file names will be returned. [Type: logical] #>     - full.names: a logical value.  If 'TRUE', the directory path is prepended to the file names to give a relative file path.  If 'FALSE', the file names (rather than paths) are returned. [Type: logical] #>     - recursive: logical.  Should the listing recurse into directories? [Type: logical] #>     - ignore.case: logical.  Should pattern-matching be case-insensitive? [Type: logical] #>     - (...) #>  #> After you call a function, wait until you receive more information. #> Use the information to decide your next steps or provide a final response. #> --- Receiving response from LLM provider: --- #> To get the files in my current directory, I will call the `list.files` function with the necessary arguments. #>  #> Here's the JSON object: #>  #> ``` #> { #>   \"function\": \"list.files\", #>   \"arguments\": { #>     \"path\": \".\" #>   } #> } #> ``` #>  #> This will list the files in the current working directory. The argument `path` is set to `\".\"`, which corresponds to the current working directory, and I'm not providing any pattern or other arguments as they are optional. #>  #> Let's see what happens next... #> --- Sending request to LLM provider (llama3.1:8b): --- #> function called: list.files #> arguments used: path = . #> result: getting_started.Rmd, getting_started.Rmd.orig, man, precompile vignettes.R, sentiment_analysis.Rmd, sentiment_analysis.Rmd.orig #> --- Receiving response from LLM provider: --- #> It looks like I have a mix of files in my current directory. #>  #> The `list.files` function returned a list of file names, which are: #>  #> * `getting_started.Rmd` #> * `getting_started.Rmd.orig` #> * `man` #> * `precompile vignettes.R` #> * `sentiment_analysis.Rmd` #> * `sentiment_analysis.Rmd.orig` #>  #> These files seem to be related to some kind of project or documentation."},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/getting_started.html","id":"code-generation-and-evaluation","dir":"Articles","previous_headings":"Prompt wraps > answer_using: Have the LLM work with tools and code","what":"Code generation and evaluation","title":"Getting started","text":"answer_using_r() provides advanced prompt wrap, various options enable LLM code generation. R code can extracted, parsed validity, optionally evaluated dedicated R session (using ‘callr’ package). prompt wrap can also set ‘tool mode’ (output_as_tool = TRUE), output R code returned LLM, can used formulate final answer.","code":"# From prompt to ggplot plot <- paste0(   \"Create a scatter plot of miles per gallon (mpg) versus\",   \" horsepower (hp) for the cars in the mtcars dataset.\",   \" Use different colors to represent the number of cylinders (cyl).\",   \" Make the plot nice and readable,\",   \" but also be creative, a little crazy, and have humour!\" ) |>   answer_using_r(     pkgs_to_use = c(\"ggplot2\"),     evaluate_code = TRUE,     return_mode = \"object\"   ) |>   send_prompt(openai) plot"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/articles/getting_started.html","id":"creating-custom-prompt-wraps","dir":"Articles","previous_headings":"Prompt wraps","what":"Creating custom prompt wraps","title":"Getting started","text":"See prompt_wrap() ‘Creating prompt wraps’ vignette information create prompt wraps.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Luka Koning. Author, maintainer, copyright holder. Tjark Van de Merwe. Author, copyright holder. Kennispunt Twente. Funder.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Koning L, Van de Merwe T (2024). tidyprompt: Prompt Large Language Models Enhance Functionality. R package version 0.0.1, https://tjarkvandemerwe.github.io/tidyprompt/, https://github.com/tjarkvandemerwe/tidyprompt.","code":"@Manual{,   title = {tidyprompt: Prompt Large Language Models and Enhance Their Functionality},   author = {Luka Koning and Tjark {Van de Merwe}},   year = {2024},   note = {R package version 0.0.1, https://tjarkvandemerwe.github.io/tidyprompt/},   url = {https://github.com/tjarkvandemerwe/tidyprompt}, }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/index.html","id":"tidyprompt","dir":"","previous_headings":"","what":"Prompt Large Language Models and Enhance Their Functionality","title":"Prompt Large Language Models and Enhance Their Functionality","text":"Easily construct prompts associated logic interacting large language models (‘LLMs’). Think ‘tidyprompt’ ‘ggplot2’ package creating prompts handling LLM interactions. ‘tidyprompt’ introduces concept prompt wraps, building blocks can use quickly turn simple prompt advanced one. Prompt wraps just modify prompt text, also add extraction validation functions applied response LLM. Moreover, functions can send feedback LLM. ‘tidyprompt’ prompt wraps, can add various features prompts define evaluated LLMs. example: structured output: Obtain structured output LLM, adhering specific type /format. Use pre-built prompt wraps R code validate. feedback & retries: Automatically provide feedback LLM output expected, allowing LLM retry answer. reasoning modes: Make LLM answer prompt specific mode, chain--thought ReAct (Reasoning Acting). function calling: Give LLM ability autonomously call R functions (‘tools’). , LLM can retrieve information take actions. ‘tidyprompt’ also supports R code generation evaluation, allowing LLMs run R code.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Prompt Large Language Models and Enhance Their Functionality","text":"Install development version GitHub: install CRAN (pending release):","code":"# install.packages(\"remotes\") remotes::install_github(\"tjarkvandemerwe/tidyprompt\") install.packages(\"tidyprompt\")"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/index.html","id":"getting-started","dir":"","previous_headings":"","what":"Getting started","title":"Prompt Large Language Models and Enhance Their Functionality","text":"See ‘Getting started’ vignette detailed introduction using ‘tidyprompt’.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/index.html","id":"examples","dir":"","previous_headings":"","what":"Examples","title":"Prompt Large Language Models and Enhance Their Functionality","text":"quick examples can ‘tidyprompt’:","code":"\"What is 5+5?\" |>   answer_as_integer() |>   send_prompt(llm_provider_ollama()) #> [1] 10 \"Are you a large language model?\" |>   answer_as_boolean() |>   send_prompt(llm_provider_ollama()) #> [1] TRUE \"What animal is the biggest?\" |>   answer_as_regex_match(\"^(cat|dog|elephant)$\") |>   send_prompt(llm_provider_ollama()) #> [1] \"elephant\" # Make LLM use a function from an R package to search Wikipedia for the answer \"What is something fun that happened in November 2024?\" |>   answer_as_text(max_words = 25) |>   answer_using_tools(getwiki::search_wiki) |>   send_prompt(llm_provider_ollama()) #> [1] \"The 2024 ARIA Music Awards ceremony, a vibrant celebration of Australian music, #> took place on November 20, 2024.\" # From prompt to linear model object in R model <- paste0(   \"Using my data, create a statistical model\",   \" investigating the relationship between two variables.\" ) |>   answer_using_r(     objects_to_use = list(data = cars),     evaluate_code = TRUE,     return_mode = \"object\"   ) |>   prompt_wrap(     validation_fn = function(x) {       if (!inherits(x, \"lm\"))         return(llm_feedback(\"The output should be a linear model object.\"))       return(TRUE)     }   ) |>   send_prompt(llm_provider_ollama()) summary(model) #> Call: #> lm(formula = speed ~ dist, data = data) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -7.5293 -2.1550  0.3615  2.4377  6.4179  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  8.28391    0.87438   9.474 1.44e-12 *** #> dist         0.16557    0.01749   9.464 1.49e-12 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 3.156 on 48 degrees of freedom #> Multiple R-squared:  0.6511, Adjusted R-squared:  0.6438  #> F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12 # Escape validation on questions that cannot be answered \"How many years old is my neighbour's dog?\" |>   answer_as_integer() |>   quit_if() |>   send_prompt(llm_provider_ollama()) #> NULL # LLM in the loop;  #   LLM verifies answer of LLM and can provide feedback \"What is the capital of France?\" |>   llm_verify() |>   send_prompt(llm_provider_ollama()) #> ...    # Human in the loop;  #   user verifies answer of LLM and can provide feedback \"What is the capital of France?\" |>   user_verify() |>   send_prompt(llm_provider_ollama()) #> ..."},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/index.html","id":"more-information-and-contributing","dir":"","previous_headings":"","what":"More information and contributing","title":"Prompt Large Language Models and Enhance Their Functionality","text":"‘tidyprompt’ active development Luka Koning (l.koning@kennispunttwente.nl) Tjark van de Merwe (t.vandemerwe@kennispunttwente.nl). Note stage, package may yet fully stable architecture may subject change. encounter issues, questions, suggestions, please open issue GitHub repository. also welcome contribute package opening pull request.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/index.html","id":"why-tidyprompt","dir":"","previous_headings":"More information and contributing","what":"Why ‘tidyprompt’?","title":"Prompt Large Language Models and Enhance Their Functionality","text":"designed ‘tidyprompt’ found writing code repeatedly construct prompts handle associated output LLMs; tasks intertwined. Often times, also wanted add features prompts, take away, required us rewrite lot code. Thus, wanted building blocks easily construct prompts simultaneously add code handle output LLMs. led us design inspired piping syntax, popularized ‘tidyverse’ familiar many R users. ‘tidyprompt’ seen tool can used enhance functionality LLMs beyond APIs natively offer. designed flexible provider-agnostic, features can used wide range LLM providers models. ‘tidyprompt’ primarily focused ‘text-based’ handling LLMs, textual output parsed achieve structured output functionalities. Several LLM providers models also offers forms ‘native’ handling, LLM directly controlled LLM provider provide output certain manner. appropriate, ‘tidyprompt’ may also support native configuration specific APIs. Currently, answer_as_json() answer_using_tools() offer native support adhering JSON schemas calling functions. Native handling may powerful cases, restrictive cases. good test works best use case. Note also prompt wraps may extend enforced native handling, adding additional validation feedback. philosophy behind ‘tidyprompt’ furthermore aims flexible enough users can implement advanced features, potentially specific certain LLM providers, within options custom prompt wraps. way, ‘tidyprompt’ can powerful tool wide range use cases, without focusing maintaining provider-specific features.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/index.html","id":"tidyprompt-versus-elmer--tidyllm","dir":"","previous_headings":"More information and contributing > Why ‘tidyprompt’?","what":"‘tidyprompt’ versus ‘elmer’ & ‘tidyllm’","title":"Prompt Large Language Models and Enhance Their Functionality","text":"line , ‘tidyprompt’ less focused interfacing APIs various LLM providers, like R packages ‘elmer’ ‘tidyllm’ . Instead, ‘tidyprompt’ primarily focused offering framework constructing prompts associated logic interactions LLMs. aim design ‘tidyprompt’ way may compatible ‘elmer’, ‘tidyllm’, packages offering interface LLM APIs. open feedback design may include compatability specific features packages future.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/add_msg_to_chat_history.html","id":null,"dir":"Reference","previous_headings":"","what":"Add a message to a chat history — add_msg_to_chat_history","title":"Add a message to a chat history — add_msg_to_chat_history","text":"function appends message chat_history() object. function can automatically determine role message added based last message chat history. role can also manually specified.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/add_msg_to_chat_history.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add a message to a chat history — add_msg_to_chat_history","text":"","code":"add_msg_to_chat_history(   chat_history,   message,   role = c(\"auto\", \"user\", \"assistant\", \"system\", \"tool\"),   tool_result = NULL )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/add_msg_to_chat_history.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add a message to a chat history — add_msg_to_chat_history","text":"chat_history single string, data.frame valid chat history (see [chat_history()]), list containing valid chat history key '$chat_history', tidyprompt-class object, NULL chat_history() object message character string representing message add role character string representing role message sender. One : \"auto\": function automatically determines role. last message user, role \"assistant\". last message anything else, role \"user\" \"user\": message user \"assistant\": message assistant \"system\": message system \"tool\": message tool (e.g., indicating result function call) tool_result logical indicating whether message tool result (e.g., result function call)","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/add_msg_to_chat_history.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add a message to a chat history — add_msg_to_chat_history","text":"chat_history() object message added last row","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/add_msg_to_chat_history.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add a message to a chat history — add_msg_to_chat_history","text":"chat_history object may different types: single string: function create new chat history object string first message; role first message \"user\" data.frame: function append message data.frame. data.frame must valid chat history; see chat_history() list: function extract chat history list. list must contain valid chat history key 'chat_history'. may typically result send_prompt() using 'return_mode = \"full\"' Tidyprompt object (tidyprompt): function extract chat history object. done concatenating 'system_prompt', 'chat_history', 'base_prompt' chat history data.frame. Note properties tidyprompt object lost NULL: function create new chat history object messages; message first message","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/add_msg_to_chat_history.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add a message to a chat history — add_msg_to_chat_history","text":"","code":"chat <- \"Hi there!\" |>   chat_history() chat #>   role   content tool_result #> 1 user Hi there!       FALSE  chat_from_df <- data.frame(   role = c(\"user\", \"assistant\"),   content = c(\"Hi there!\", \"Hello! How can I help you today?\") ) |>   chat_history() chat_from_df #>        role                          content tool_result #> 1      user                        Hi there!       FALSE #> 2 assistant Hello! How can I help you today?       FALSE  # `add_msg_to_chat_history()` may be used to add messages to a chat history chat_from_df <- chat_from_df |>   add_msg_to_chat_history(\"Calculate 2+2 for me, please!\") chat_from_df #>        role                          content tool_result #> 1      user                        Hi there!       FALSE #> 2 assistant Hello! How can I help you today?       FALSE #> 3      user    Calculate 2+2 for me, please!       FALSE  # You can also continue conversations which originate from `send_prompt()`: if (FALSE) { # \\dontrun{   result <- \"Hi there!\" |>     send_prompt(return_mode = \"full\")   # --- Sending request to LLM provider (llama3.1:8b): ---   # Hi there!   # --- Receiving response from LLM provider: ---   # It's nice to meet you. Is there something I can help you with, or would you   # like to chat?    # Access the chat history from the result:   chat_from_send_prompt <- result$chat_history    # Add a message to the chat history:   chat_history_with_new_message <- chat_from_send_prompt |>     add_msg_to_chat_history(\"Let's chat!\")    # The new chat history can be input for a new tidyprompt:   prompt <- tidyprompt(chat_history_with_new_message)    # You can also take an existing tidyprompt and add the new chat history to it;   #   this way, you can continue a conversation using the same prompt wraps   prompt$set_chat_history(chat_history_with_new_message)    # send_prompt() also accepts a chat history as input:   new_result <- chat_history_with_new_message |>     send_prompt(return_mode = \"full\")    # You can also create a persistent chat history object from   #   a chat history data frame; see ?`persistent_chat-class`   chat <- `persistent_chat-class`$new(llm_provider_ollama(), chat_from_send_prompt)   chat$chat(\"Let's chat!\") } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/add_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Add text to a tidyprompt — add_text","title":"Add text to a tidyprompt — add_text","text":"Add text prompt adding prompt_wrap() append text current prompt text.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/add_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add text to a tidyprompt — add_text","text":"","code":"add_text(prompt, text, position = c(\"after\", \"before\"), sep = \"\\n\\n\")"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/add_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add text to a tidyprompt — add_text","text":"prompt single string tidyprompt() object text Text added current prompt text position add text; either \"\" \"\". sep Separator used current prompt text text added","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/add_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add text to a tidyprompt — add_text","text":"tidyprompt() added prompt_wrap() append text end current prompt text","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/add_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add text to a tidyprompt — add_text","text":"","code":"prompt <- \"Hi there!\" |>   add_text(\"How is your day?\") prompt #> <tidyprompt> #> The base prompt is modified by a prompt wrap, resulting in: #> > Hi there! #> >  #> > How is your day?  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #> Use 'get_prompt_wraps(x)' to show the prompt wraps. #>  prompt |>   construct_prompt_text() #> [1] \"Hi there!\\n\\nHow is your day?\""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_boolean.html","id":null,"dir":"Reference","previous_headings":"","what":"Make LLM answer as a boolean (TRUE or FALSE) — answer_as_boolean","title":"Make LLM answer as a boolean (TRUE or FALSE) — answer_as_boolean","text":"Make LLM answer boolean (TRUE FALSE)","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_boolean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make LLM answer as a boolean (TRUE or FALSE) — answer_as_boolean","text":"","code":"answer_as_boolean(   prompt,   true_definition = NULL,   false_definition = NULL,   add_instruction_to_prompt = TRUE )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_boolean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make LLM answer as a boolean (TRUE or FALSE) — answer_as_boolean","text":"prompt single string tidyprompt() object true_definition (optional) Definition constitute TRUE. included instruction LLM. single string false_definition (optional) Definition constitute FALSE. included instruction LLM. single string add_instruction_to_prompt (optional) Add instruction replying boolean prompt text. Set FALSE debugging extractions/validations working expected (without instruction answer fail validation function, initiating retry)","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_boolean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make LLM answer as a boolean (TRUE or FALSE) — answer_as_boolean","text":"tidyprompt() added prompt_wrap() ensure LLM response boolean","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_boolean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make LLM answer as a boolean (TRUE or FALSE) — answer_as_boolean","text":"","code":"if (FALSE) { # \\dontrun{   \"Are you a large language model?\" |>     answer_as_boolean() |>     send_prompt()   # --- Sending request to LLM provider (llama3.1:8b): ---   #   Are you a large language model?   #   #   You must answer with only TRUE or FALSE (use no other characters).   # --- Receiving response from LLM provider: ---   #   TRUE   # [1] TRUE } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_integer.html","id":null,"dir":"Reference","previous_headings":"","what":"Make LLM answer as an integer (between min and max) — answer_as_integer","title":"Make LLM answer as an integer (between min and max) — answer_as_integer","text":"Make LLM answer integer (min max)","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_integer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make LLM answer as an integer (between min and max) — answer_as_integer","text":"","code":"answer_as_integer(   prompt,   min = NULL,   max = NULL,   add_instruction_to_prompt = TRUE )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_integer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make LLM answer as an integer (between min and max) — answer_as_integer","text":"prompt single string tidyprompt() object min (optional) Minimum value integer max (optional) Maximum value integer add_instruction_to_prompt (optional) Add instruction replying integer prompt text. Set FALSE debugging extractions/validations working expected (without instruction answer fail validation function, initiating retry)","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_integer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make LLM answer as an integer (between min and max) — answer_as_integer","text":"tidyprompt() added prompt_wrap() ensure LLM response integer.","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_integer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make LLM answer as an integer (between min and max) — answer_as_integer","text":"","code":"if (FALSE) { # \\dontrun{   \"What is 5 + 5?\" |>     answer_as_integer() |>     send_prompt()   # --- Sending request to LLM provider (llama3.1:8b): ---   #   What is 5 + 5?   #   #   You must answer with only an integer (use no other characters).   # --- Receiving response from LLM provider: ---   #   10   # [1] 10 } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_json.html","id":null,"dir":"Reference","previous_headings":"","what":"Make LLM answer as JSON (with optional schema) — answer_as_json","title":"Make LLM answer as JSON (with optional schema) — answer_as_json","text":"functions wraps prompt settings ensure LLM response valid JSON object, optionally matching given JSON schema. function can work models providers text-based handling, also supports native settings OpenAI Ollama API types. (See argument 'type'.) means possible easily switch providers different levels JSON support, ensuring results correct format.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_json.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make LLM answer as JSON (with optional schema) — answer_as_json","text":"","code":"answer_as_json(   prompt,   schema = NULL,   schema_strict = FALSE,   schema_in_prompt_as = c(\"example\", \"schema\"),   type = c(\"text-based\", \"auto\", \"openai\", \"ollama\", \"openai_oo\", \"ollama_oo\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_json.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make LLM answer as JSON (with optional schema) — answer_as_json","text":"prompt single string tidyprompt() object schema list represents JSON schema response match.See example API's documentation information defining JSON schemas. Note schema list (R object) representing JSON schema, JSON string (use jsonlite::fromJSON() jsonlite::toJSON() convert two) schema_strict TRUE, provided schema strictly enforced. option passed part schema using type  type \"openai\" \"ollama\", using types passed jsonvalidate::json_validate() schema_in_prompt_as providing schema using type \"text-based\", \"openai_oo\", \"ollama_oo\", argument specifies schema included prompt: \"example\" (default): schema included example JSON object (tends work best). r_json_schema_to_example() used generate example object schema \"schema\": schema included JSON schema type way JSON response enforced: \"text-based\": Instruction added prompt asking JSON; schema provided, also included prompt (see argument 'schema_in_prompt_as'). JSON parsed LLM response , schema provided, validated schema jsonvalidate::json_validate(). Feedback sent LLM response valid. option always works, may cases may less powerful native JSON options \"auto\": Automatically determine type based 'llm_provider$api_type'. consider model compatibility lead errors; set 'type' manually errors occur; use 'text-based' unsure \"openai\" \"ollama\": response format set via relevant API parameters, making API enforce valid JSON response. schema provided, also included API parameters also enforced API. schema provided, request JSON added prompt (required APIs). Note JSON options may available models provider; consult documentation information. unsure encounter errors, use \"text-based\" \"openai_oo\" \"ollama_oo\": Similar \"openai\" \"ollama\", schema provided included API parameters. Schema validation done R jsonvalidate::json_validate(). can useful want use API's JSON support, schema support limited Note \"openai\" \"ollama\" types may also work APIs similar structure","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_json.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make LLM answer as JSON (with optional schema) — answer_as_json","text":"tidyprompt() added prompt_wrap() ensure LLM response valid JSON object","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_json.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make LLM answer as JSON (with optional schema) — answer_as_json","text":"","code":"base_prompt <- \"How can I solve 8x + 7 = -23?\"  # This example will show how to enforce JSON format in the response, #   with and without a schema, using the 'answer_as_json()' prompt wrap. # If you use type = 'auto', the function will automatically detect the #   best way to enforce JSON based on the LLM provider you are using. # Note that the default type is 'text-based', which will work for any provider/model  #### Enforcing JSON without a schema: ####  if (FALSE) { # \\dontrun{   ## Text-based (works for any provider/model):   #   Adds request to prompt for a JSON object   #   Extracts JSON from textual response (feedback for retry if no JSON received)   #   Parses JSON to R object   json_1 <- base_prompt |>     answer_as_json() |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   # How can I solve 8x + 7 = -23?   #   # Your must format your response as a JSON object.   # --- Receiving response from LLM provider: ---   # Here is the solution to the equation formatted as a JSON object:   #   # ```   # {   #   \"equation\": \"8x + 7 = -23\",   #   \"steps\": [   #     {   #       \"step\": \"Subtract 7 from both sides of the equation\",   #       \"expression\": \"-23 - 7\"   #     },   #     {   #       \"step\": \"Simplify the expression on the left side\",   #       \"result\": \"-30\"   #     },   #     {   #       \"step\": \"Divide both sides by -8 to solve for x\",   #       \"expression\": \"-30 / -8\"   #     },   #     {   #       \"step\": \"Simplify the expression on the right side\",   #       \"result\": \"3.75\"   #     }   #   ],   #   \"solution\": {   #     \"x\": 3.75   #   }   # }   # ```     ## Ollama:   #   - Sets 'format' parameter to 'json', enforcing JSON   #   - Adds request to prompt for a JSON object, as is recommended by the docs   #   - Parses JSON to R object   json_2 <- base_prompt |>     answer_as_json(type = \"auto\") |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   # How can I solve 8x + 7 = -23?   #   # Your must format your response as a JSON object.   # --- Receiving response from LLM provider: ---   # {\"steps\": [   #   \"Subtract 7 from both sides to get 8x = -30\",   #   \"Simplify the right side of the equation to get 8x = -30\",   #   \"Divide both sides by 8 to solve for x, resulting in x = -30/8\",   #   \"Simplify the fraction to find the value of x\"   # ],   # \"value_of_x\": \"-3.75\"}     ## OpenAI-type API without schema:   #   - Sets 'response_format' parameter to 'json_object', enforcing JSON   #   - Adds request to prompt for a JSON object, as is required by the API   #   - Parses JSON to R object   json_3 <- base_prompt |>     answer_as_json(type = \"auto\") |>     send_prompt(llm_provider_openai())   # --- Sending request to LLM provider (gpt-4o-mini): ---   # How can I solve 8x + 7 = -23?   #   # Your must format your response as a JSON object.   # --- Receiving response from LLM provider: ---   # {   #   \"solution_steps\": [   #     {   #       \"step\": 1,   #       \"operation\": \"Subtract 7 from both sides\",   #       \"equation\": \"8x + 7 - 7 = -23 - 7\",   #       \"result\": \"8x = -30\"   #     },   #     {   #       \"step\": 2,   #       \"operation\": \"Divide both sides by 8\",   #       \"equation\": \"8x / 8 = -30 / 8\",   #       \"result\": \"x = -3.75\"   #     }   #   ],   #   \"solution\": {   #     \"x\": -3.75   #   }   # } } # }    #### Enforcing JSON with a schema: ####  # Make a list representing a JSON schema, #   which the LLM response must adhere to: json_schema <- list(   name = \"steps_to_solve\", # Required for OpenAI API   description = NULL, # Optional for OpenAI API   schema = list(     type = \"object\",     properties = list(       steps = list(         type = \"array\",         items = list(           type = \"object\",           properties = list(             explanation = list(type = \"string\"),             output = list(type = \"string\")           ),           required = c(\"explanation\", \"output\"),           additionalProperties = FALSE         )       ),       final_answer = list(type = \"string\")     ),     required = c(\"steps\", \"final_answer\"),     additionalProperties = FALSE   )   # 'strict' parameter is set as argument 'answer_as_json()' ) # Note: when you are not using an OpenAI API, you can also pass just the #   internal 'schema' list object to 'answer_as_json()' instead of the full #   'json_schema' list object  # Generate example R object based on schema: r_json_schema_to_example(json_schema) #> $steps #> $steps[[1]] #> $steps[[1]]$explanation #> [1] \"...\" #>  #> $steps[[1]]$output #> [1] \"...\" #>  #>  #>  #> $final_answer #> [1] \"...\" #>   if (FALSE) { # \\dontrun{   ## Text-based with schema (works for any provider/model):   #   - Adds request to prompt for a JSON object   #   - Adds schema to prompt   #   - Extracts JSON from textual response (feedback for retry if no JSON received)   #   - Validates JSON against schema with 'jsonvalidate' package (feedback for retry if invalid)   #   - Parses JSON to R object   json_4 <- base_prompt |>     answer_as_json(schema = json_schema) |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   # How can I solve 8x + 7 = -23?   #   # Your must format your response as a JSON object.   #   # Your JSON object should match this example JSON object:   #   {   #     \"steps\": [   #       {   #         \"explanation\": \"...\",   #         \"output\": \"...\"   #       }   #     ],   #     \"final_answer\": \"...\"   #   }   # --- Receiving response from LLM provider: ---   # Here is the solution to the equation:   #   # ```   # {   #   \"steps\": [   #     {   #       \"explanation\": \"First, we want to isolate the term with 'x' by   #       subtracting 7 from both sides of the equation.\",   #       \"output\": \"8x + 7 - 7 = -23 - 7\"   #     },   #     {   #       \"explanation\": \"This simplifies to: 8x = -30\",   #       \"output\": \"8x = -30\"   #     },   #     {   #       \"explanation\": \"Next, we want to get rid of the coefficient '8' by   #       dividing both sides of the equation by 8.\",   #       \"output\": \"(8x) / 8 = (-30) / 8\"   #     },   #     {   #       \"explanation\": \"This simplifies to: x = -3.75\",   #       \"output\": \"x = -3.75\"   #     }   #   ],   #   \"final_answer\": \"-3.75\"   # }   # ```     ## Ollama with schema:   #   - Sets 'format' parameter to 'json', enforcing JSON   #   - Adds request to prompt for a JSON object, as is recommended by the docs   #   - Adds schema to prompt   #   - Validates JSON against schema with 'jsonvalidate' package (feedback for retry if invalid)   json_5 <- base_prompt |>     answer_as_json(json_schema, type = \"auto\") |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   # How can I solve 8x + 7 = -23?   #   # Your must format your response as a JSON object.   #   # Your JSON object should match this example JSON object:   # {   #   \"steps\": [   #     {   #       \"explanation\": \"...\",   #       \"output\": \"...\"   #     }   #   ],   #   \"final_answer\": \"...\"   # }   # --- Receiving response from LLM provider: ---   # {   #   \"steps\": [   #     {   #       \"explanation\": \"First, subtract 7 from both sides of the equation to   #       isolate the term with x.\",   #       \"output\": \"8x = -23 - 7\"   #     },   #     {   #       \"explanation\": \"Simplify the right-hand side of the equation.\",   #       \"output\": \"8x = -30\"   #     },   #     {   #       \"explanation\": \"Next, divide both sides of the equation by 8 to solve for x.\",   #       \"output\": \"x = -30 / 8\"   #     },   #     {   #       \"explanation\": \"Simplify the right-hand side of the equation.\",   #       \"output\": \"x = -3.75\"   #     }   #   ],   #   \"final_answer\": \"-3.75\"   # }    ## OpenAI with schema:   #   - Sets 'response_format' parameter to 'json_object', enforcing JSON   #   - Adds json_schema to the API request, API enforces JSON adhering schema   #   - Parses JSON to R object   json_6 <- base_prompt |>     answer_as_json(json_schema, type = \"auto\") |>     send_prompt(llm_provider_openai())   # --- Sending request to LLM provider (gpt-4o-mini): ---   # How can I solve 8x + 7 = -23?   # --- Receiving response from LLM provider: ---   # {\"steps\":[   # {\"explanation\":\"Start with the original equation.\",   # \"output\":\"8x + 7 = -23\"},   # {\"explanation\":\"Subtract 7 from both sides to isolate the term with x.\",   # \"output\":\"8x + 7 - 7 = -23 - 7\"},   # {\"explanation\":\"Simplify the left side and the right side of the equation.\",   # \"output\":\"8x = -30\"},   # {\"explanation\":\"Now, divide both sides by 8 to solve for x.\",   # \"output\":\"x = -30 / 8\"},   # {\"explanation\":\"Simplify the fraction by dividing both the numerator and the   # denominator by 2.\",   # \"output\":\"x = -15 / 4\"}   # ], \"final_answer\":\"x = -15/4\"} } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_key_value.html","id":null,"dir":"Reference","previous_headings":"","what":"Make LLM answer as a list of key-value pairs — answer_as_key_value","title":"Make LLM answer as a list of key-value pairs — answer_as_key_value","text":"function similar answer_as_list() instead returning list items, instructs LLM return list key-value pairs.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_key_value.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make LLM answer as a list of key-value pairs — answer_as_key_value","text":"","code":"answer_as_key_value(   prompt,   key_name = \"key\",   value_name = \"value\",   pair_explanation = NULL,   n_unique_items = NULL,   list_mode = c(\"bullet\", \"comma\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_key_value.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make LLM answer as a list of key-value pairs — answer_as_key_value","text":"prompt single string tidyprompt() object key_name (optional) name placeholder describing \"key\" part pair value_name (optional) name placeholder describing \"value\" part pair pair_explanation (optional) Additional explanation pair . single string. appended list instruction. n_unique_items (optional) Number unique key-value pairs required list list_mode (optional) Mode list: \"bullet\" \"comma\". \"bullet\" mode expects pairs like:   \"comma\" mode expects pairs like:","code":"-- key1: value1 -- key2: value2 1. key: value, 2. key: value, etc."},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_key_value.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make LLM answer as a list of key-value pairs — answer_as_key_value","text":"tidyprompt() added prompt_wrap() ensure LLM response list key-value pairs.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_key_value.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make LLM answer as a list of key-value pairs — answer_as_key_value","text":"","code":"if (FALSE) { # \\dontrun{   \"What are a few capital cities around the world?\" |>     answer_as_key_value(       key_name = \"country\",       value_name = \"capital\"     ) |>     send_prompt()   # --- Sending request to LLM provider (llama3.1:8b): ---   # What are a few capital cities around the world?   #   # Respond with a list of key-value pairs, like so:   #   -- <<country 1>>: <<capital 1>>   #   -- <<country 2>>: <<capital 2>>   #   etc.   # --- Receiving response from LLM provider: ---   # Here are a few:   #   -- Australia: Canberra   #   -- France: Paris   #   -- United States: Washington D.C.   #   -- Japan: Tokyo   #   -- China: Beijing   # $Australia   # [1] \"Canberra\"   #   # $France   # [1] \"Paris\"   #   # $`United States`   # [1] \"Washington D.C.\"   #   # $Japan   # [1] \"Tokyo\"   #   # $China   # [1] \"Beijing\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Make LLM answer as a list of items — answer_as_list","title":"Make LLM answer as a list of items — answer_as_list","text":"Make LLM answer list items","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make LLM answer as a list of items — answer_as_list","text":"","code":"answer_as_list(   prompt,   item_name = \"item\",   item_explanation = NULL,   n_unique_items = NULL,   list_mode = c(\"bullet\", \"comma\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make LLM answer as a list of items — answer_as_list","text":"prompt single string tidyprompt() object item_name (optional) Name items list item_explanation (optional) Additional explanation item . Item explanation single string. appended list instruction n_unique_items (optional) Number unique items required list list_mode (optional) Mode list. Either \"bullet\" \"comma\". \"bullet mode expects items listed \"–\" item, new line item (e.g., \"– item1\\n– item2\\n– item3\"). \"comma\" mode expects items listed number period (e.g., \"1. item1, 2. item2, 3. item3\"). \"comma\" mode may easier smaller LLMs use","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make LLM answer as a list of items — answer_as_list","text":"tidyprompt() added prompt_wrap() ensure LLM response list items","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make LLM answer as a list of items — answer_as_list","text":"","code":"if (FALSE) { # \\dontrun{   \"What are some delicious fruits?\" |>     answer_as_list(item_name = \"fruit\", n_unique_items = 5) |>     send_prompt()   # --- Sending request to LLM provider (llama3.1:8b): ---   # What are some delicious fruits?   #   # Respond with a list, like so:   #   -- <<fruit 1>>   #   -- <<fruit 2>>   #   etc.   # The list should contain 5 unique items.   # --- Receiving response from LLM provider: ---   # Here's the list of delicious fruits:   #   -- Strawberries   #   -- Pineapples   #   -- Mangoes   #   -- Blueberries   #   -- Pomegranates   # [[1]]   # [1] \"Strawberries\"   #   # [[2]]   # [1] \"Pineapples\"   #   # [[3]]   # [1] \"Mangoes\"   #   # [[4]]   # [1] \"Blueberries\"   #   # [[5]]   # [1] \"Pomegranates\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_named_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Make LLM answer as a named list — answer_as_named_list","title":"Make LLM answer as a named list — answer_as_named_list","text":"Get named list LLM response optional item instructions validations.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_named_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make LLM answer as a named list — answer_as_named_list","text":"","code":"answer_as_named_list(   prompt,   item_names,   item_instructions = NULL,   item_validations = NULL )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_named_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make LLM answer as a named list — answer_as_named_list","text":"prompt single string tidyprompt() object item_names character vector specifying expected item names item_instructions optional named list additional instructions item item_validations optional named list validation functions item. Like validation functions prompt_wrap(), functions return llm_feedback() validation fails. validation successful, function return TRUE","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_named_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make LLM answer as a named list — answer_as_named_list","text":"tidyprompt() added prompt_wrap() ensures LLM response named list specified item names, optional instructions, validations.","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_named_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make LLM answer as a named list — answer_as_named_list","text":"","code":"if (FALSE) { # \\dontrun{   persona <- \"Create a persona for me, please.\" |>     answer_as_named_list(       item_names = c(\"name\", \"age\", \"occupation\"),       item_instructions = list(         name = \"The name of the persona\",         age = \"The age of the persona\",         occupation = \"The occupation of the persona\"       )     ) |> send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   #   Create a persona for me, please.   #   #   Respond with a named list like so:   #     -- name: <<value>> (The name of the persona)   #     -- age: <<value>> (The age of the persona)   #     -- occupation: <<value>> (The occupation of the persona)   #   Each name must correspond to: name, age, occupation   # --- Receiving response from LLM provider: ---   #   Here is your persona:   #   #   -- name: Astrid Welles   #   -- age: 32   #   -- occupation: Museum Curator   persona$name   # [1] \"Astrid Welles\"   persona$age   # [1] \"32\"   persona$occupation   # [1] \"Museum Curator\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_regex_match.html","id":null,"dir":"Reference","previous_headings":"","what":"Make LLM answer match a specific regex — answer_as_regex_match","title":"Make LLM answer match a specific regex — answer_as_regex_match","text":"Make LLM answer match specific regex","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_regex_match.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make LLM answer match a specific regex — answer_as_regex_match","text":"","code":"answer_as_regex_match(prompt, regex, mode = c(\"full_match\", \"extract_matches\"))"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_regex_match.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make LLM answer match a specific regex — answer_as_regex_match","text":"prompt single string tidyprompt() object regex character string specifying regular expression response must match mode character string specifying mode regex match. Options \"exact_match\" (default) \"extract_all_matches\". \"full_match\", full LLM response must match regex. , LLM sent feedback retry. full LLM response returned regex matched. \"extract_matches\", matches regex LLM response returned (present). regex matched , LLM sent feedback retry. least one match, matches returned character vector","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_regex_match.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make LLM answer match a specific regex — answer_as_regex_match","text":"tidyprompt() added prompt_wrap() ensure LLM response matches specified regex","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_regex_match.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make LLM answer match a specific regex — answer_as_regex_match","text":"","code":"if (FALSE) { # \\dontrun{   \"What would be a suitable e-mail address for cupcake company?\" |>     answer_as_regex_match(\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\") |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   #   What would be a suitable e-mail address for cupcake company?   #   #   You must answer with a response that matches this regex format:   #     ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$   #     (use no other characters)   # --- Receiving response from LLM provider: ---   #   sweet.treats.cupcakes@gmail.com   # [1] \"sweet.treats.cupcakes@gmail.com\"    \"What would be a suitable e-mail address for cupcake company?\" |>     add_text(\"Give three ideas.\") |>     answer_as_regex_match(       \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}\",       mode = \"extract_matches\"     ) |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   #   What would be a suitable e-mail address for cupcake company?   #   #   Give three ideas.   #   #   You must answer with a response that matches this regex format:   #     [a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}   # --- Receiving response from LLM provider: ---   #   Here are three potential email addresses for a cupcake company:   #   #   1. sweettreats.cupcakes@yummail.com   #   2. cupcakes.and.love@flourpower.net   #   3. thecupcakery@gmail.com   # [1] \"sweettreats.cupcakes@yummail.com\" \"cupcakes.and.love@flourpower.net\"   # \"thecupcakery@gmail.com\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Make LLM answer as a constrained text response — answer_as_text","title":"Make LLM answer as a constrained text response — answer_as_text","text":"Make LLM answer constrained text response","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make LLM answer as a constrained text response — answer_as_text","text":"","code":"answer_as_text(   prompt,   max_words = NULL,   max_characters = NULL,   add_instruction_to_prompt = TRUE )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make LLM answer as a constrained text response — answer_as_text","text":"prompt single string tidyprompt() object max_words (optional) Maximum number words allowed response. specified, responses exceeding limit fail validation max_characters (optional) Maximum number characters allowed response. specified, responses exceeding limit fail validation add_instruction_to_prompt (optional) Add instruction replying within constraints prompt text. Set FALSE debugging extractions/validations working expected (without instruction answer fail validation function, initiating retry)","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make LLM answer as a constrained text response — answer_as_text","text":"tidyprompt() added prompt_wrap() ensure LLM response conforms specified constraints","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_as_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make LLM answer as a constrained text response — answer_as_text","text":"","code":"if (FALSE) { # \\dontrun{   \"What is a large language model?\" |>     answer_as_text(max_words = 10) |>     send_prompt()   # --- Sending request to LLM provider (llama3.1:8b): ---   # What is a large language model?   #   # You must provide a text response. The response must be at most 10 words.   # --- Receiving response from LLM provider: ---   # A type of AI that processes and generates human-like text.   # [1] \"A type of AI that processes and generates human-like text.\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_by_chain_of_thought.html","id":null,"dir":"Reference","previous_headings":"","what":"Set chain of thought mode for a prompt — answer_by_chain_of_thought","title":"Set chain of thought mode for a prompt — answer_by_chain_of_thought","text":"function enables chain thought mode evaluation prompt tidyprompt(). chain thought mode, large language model (LLM) chain thought mode, large language model (LLM) asked think step step arrive final answer. hypothesized may increase LLM performance solving complex tasks. Chain thought mode inspired method described Wei et al. (2022).","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_by_chain_of_thought.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set chain of thought mode for a prompt — answer_by_chain_of_thought","text":"","code":"answer_by_chain_of_thought(   prompt,   extract_from_finish_brackets = TRUE,   extraction_lenience = TRUE )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_by_chain_of_thought.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set chain of thought mode for a prompt — answer_by_chain_of_thought","text":"prompt single string tidyprompt() object extract_from_finish_brackets logical indicating whether final answer extracted text inside \"FINISH...\" brackets. extraction_lenience logical indcating whether extraction function lenient. TRUE, extraction function attempt extract final answer even extracted within brackets, extracting everything final occurence 'FINISH' (present). may useful smaller LLMs may follow output format strictly","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_by_chain_of_thought.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set chain of thought mode for a prompt — answer_by_chain_of_thought","text":"tidyprompt() added prompt_wrap() ensure LLM follows chain thought mode answering prompt","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_by_chain_of_thought.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Set chain of thought mode for a prompt — answer_by_chain_of_thought","text":"Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & Zhou, D. (2022). Chain--Thought Prompting Elicits Reasoning Large Language Models. doi:10.48550/arXiv.2201.11903","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_by_chain_of_thought.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set chain of thought mode for a prompt — answer_by_chain_of_thought","text":"","code":"if (FALSE) { # \\dontrun{   \"What is 5 + 5?\" |>     answer_by_chain_of_thought() |>     answer_as_integer() |>     send_prompt()   # --- Sending request to LLM provider (llama3.1:8b): ---   # You are given a user's prompt.   # To answer the user's prompt, you need to think step by step to arrive at a final answer.   #   # ----- START OF USER'S PROMPT -----   # What is 5 + 5?   #   # You must answer with only an integer (use no other characters).   # ----- END OF USER'S PROMPT -----   #   # What are the steps you would take to answer the user's prompt?   # Describe your thought process in the following format:   #   >> step 1: <step 1 description>   #   >> step 2: <step 2 description>   #   (etc.)   #   # When you are done, you must type:   #   FINISH[<put here your final answer to the user's prompt>]   #   # Make sure your final answer follows the logical conclusion of your thought process.   # --- Receiving response from LLM provider: ---   # >> step 1: Recognize that the user's prompt is a simple arithmetic question asking for   # the sum of two numbers.   #   # >> step 2: Identify the specific numbers mentioned in the prompt, which are 5 and 5.   #   # >> step 3: Recall basic addition facts from memory or knowledge.   # In this case, knowing that 5 + 0 = 5, 5 + 1 = 6, 5 + 2 = 7, and so on up to 5 + 4 = 9,   # it is clear that the next step in adding 5 to itself would be to keep the same sum.   #   # >> step 4: Apply the addition principle by considering the original number (5) as being   # added to itself once more. Since 5 + 1 = 6 and the process of adding 1 increases the total   # sum, then 5 + 2 must equal 7, thus indicating a direct progression in the sum as the additive   # amount increases.   #   # >> step 5: From steps 3 and 4, conclude that since 5 added to itself (once) would be 10,   # then when it is added again (to itself), it would continue from the last result which was   # achieved by adding one more than the original number. Therefore, it must follow the pattern   # established in step 3 where each additional amount increases the total sum.   #   # >> step 6: Now knowing that the operation of adding a second 5 to the first one results in   # increasing the sum to the next integer after 10 (based on steps 3 and 4), determine what the   # next integer is after 10. Knowing that integers are whole numbers without fractions, it's   # clear that if you add another unit to 10, it becomes 11.   #   # FINISH[10]   # [1] 10 } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_by_react.html","id":null,"dir":"Reference","previous_headings":"","what":"Set ReAct mode for a prompt — answer_by_react","title":"Set ReAct mode for a prompt — answer_by_react","text":"function enables ReAct mode evaluation prompt tidyprompt(). ReAct mode, large language model (LLM) asked think step step, time detailing thought, action, observation, eventually arrive final answer. hypothesized may increase LLM performance solving complex tasks. ReAct mode inspired method described Yao et al. (2022).","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_by_react.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set ReAct mode for a prompt — answer_by_react","text":"","code":"answer_by_react(   prompt,   extract_from_finish_brackets = TRUE,   extraction_lenience = TRUE )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_by_react.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set ReAct mode for a prompt — answer_by_react","text":"prompt single string tidyprompt() object extract_from_finish_brackets logical indicating whether final answer extracted text inside \"FINISH...\" brackets extraction_lenience logical indcating whether extraction function lenient. TRUE, extraction function attempt extract final answer even extracted within brackets, extracting everything final occurence 'FINISH' (present). may useful smaller LLMs may follow output format strictly","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_by_react.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set ReAct mode for a prompt — answer_by_react","text":"tidyprompt() added prompt_wrap() ensure LLM follows ReAct mode answering prompt","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_by_react.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set ReAct mode for a prompt — answer_by_react","text":"Please note ReAct mode may useful combination tools LLM can use. See, example, 'add_tools()' enabling R function calling, , example, 'answer_as_code()' 'output_as_tool = TRUE' enabling R code evaluation tool.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_by_react.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Set ReAct mode for a prompt — answer_by_react","text":"Yao, S., Wu, Y., Cheung, W., Wang, Z., Narasimhan, K., & Kong, L. (2022). ReAct: Synergizing Reasoning Acting Language Models. doi:10.48550/arXiv.2210.03629","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_by_react.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set ReAct mode for a prompt — answer_by_react","text":"","code":"if (FALSE) { # \\dontrun{   \"What is 5 + 5?\" |>     answer_by_react() |>     answer_as_integer() |>     send_prompt()   # --- Sending request to LLM provider (llama3.1:8b): ---   # You are given a user's prompt.   # To answer the user's prompt, you need to think step by step,   # take an action if needed, and then return a final answer.   #   # ----- START OF USER'S PROMPT -----   # What is 5 + 5?   #   # You must answer with only an integer (use no other characters).   # ----- END OF USER'S PROMPT -----   #   # Use the following structure:   #   Thought: <describe your thought process>   #   Action: <if needed, describe the action you take (e.g., look up information)>   #   Observation: <describe the result or observation from the action>   # (Repeat Thought -> Action -> Observation as necessary)   #   # When you are done, you must type:   #   FINISH[<put here your final answer to the user's prompt>]   #   # Ensure your final answer aligns with your reasoning and observations.   # --- Receiving response from LLM provider: ---   # Thought: The problem is asking for the sum of two numbers, 5 and 5.   #   # Action: None needed, as this is a simple arithmetic operation that can be performed mentally.   #   # Observation: I can easily add these two numbers together in my mind to get the result.   #   # Thought: To find the sum, I will simply add the two numbers together: 5 + 5 = ?   #   # Action: Perform the addition.   #   # Observation: The result of adding 5 and 5 is 10.   #   # FINISH[10]   # [1] 10 } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_r.html","id":null,"dir":"Reference","previous_headings":"","what":"Enable LLM to draft and execute R code — answer_using_r","title":"Enable LLM to draft and execute R code — answer_using_r","text":"function adds prompt wrap tidyprompt() instructs LLM answer prompt R code. various options customize behavior prompt wrap, concerning evaluation R code, packages may used, objects already exist R session, console output sent back LLM.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_r.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enable LLM to draft and execute R code — answer_using_r","text":"","code":"answer_using_r(   prompt,   add_text = \"You must code in the programming language 'R' to answer this prompt.\",   pkgs_to_use = c(),   objects_to_use = list(),   list_packages = TRUE,   list_objects = TRUE,   skim_dataframes = TRUE,   evaluate_code = FALSE,   r_session_options = list(),   output_as_tool = FALSE,   return_mode = c(\"full\", \"code\", \"console\", \"object\", \"formatted_output\", \"llm_answer\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_r.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Enable LLM to draft and execute R code — answer_using_r","text":"prompt single string tidyprompt() object add_text Single string added prompt text, informing LLM must code R answer prompt pkgs_to_use character vector package names may used R code LLM generate. evaluating R code, packages pre-loaded R session objects_to_use named list objects may used R code LLM generate. evaluating R code, objects pre-loaded R session. names list used object names R session list_packages Logical indicating whether LLM informed packages may used R code (TRUE, list loaded packages shown initial prompt) list_objects Logical indicating whether LLM informed existence 'objects_to_use' (TRUE, list objects plus types shown initial prompt) skim_dataframes Logical indicating whether LLM informed structure dataframes present 'objects_to_use' (TRUE, skim summary data.frame type object shown initial prompt). uses function skim_with_labels_and_levels() evaluate_code Logical indicating whether R code evaluated. TRUE, R code evaluated separate R session (using 'callr' create isolated R session via r_session). Note setting 'TRUE' means code generated LLM run system; use setting caution r_session_options list options pass r_session. can used customize R session. See r_session_options available options. options provided, default options used 'system_profile' 'user_profile' set FALSE output_as_tool Logical indicating whether console output evaluated R code sent back LLM, meaning LLM use R code tool formulate final answer prompt. TRUE, LLM can decide can answer prompt output, need modify R code. LLM provide new R code (.e., prompt answered) prompt wrap end (continue long LLM provides R code). option enabled, resulting prompt_wrap() type 'tool'. TRUE, return mode also always set 'llm_answer' return_mode Single string indicating return mode. One : 'full': Return list final LLM answer, extracted R code, (argument 'evaluate_code' TRUE) output R code 'code': Return extracted R code 'console': Return console output evaluated R code 'object': Return object produced evaluated R code 'formatted_output': Return formatted string extracted R code console output, print last object (identical presented LLM 'output_as_tool' TRUE) 'llm_answer': Return final LLM answer choosing 'console' 'object', additional instruction added prompt text inform LLM expected output R code. 'output_as_tool' TRUE, return mode always set 'llm_answer' (LLM using R code tool answer prompt)","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_r.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Enable LLM to draft and execute R code — answer_using_r","text":"tidyprompt() object prompt_wrap() added , handle R code generation possibly evaluation","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_r.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Enable LLM to draft and execute R code — answer_using_r","text":"evaluation R code, 'callr' package required. Please note: automatic evaluation generated R code may dangerous system; must use function caution.","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_r.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Enable LLM to draft and execute R code — answer_using_r","text":"","code":"if (FALSE) { # \\dontrun{   # Prompt to value calculated with R   avg_miles_per_gallon <- paste0(     \"Using my data,\",     \" calculate the average miles per gallon (mpg) for cars with 6 cylinders.\"   ) |>     answer_as_integer() |>     answer_using_r(       pkgs_to_use = c(\"dplyr\"),       objects_to_use = list(mtcars = mtcars),       evaluate_code = TRUE,       output_as_tool = TRUE     ) |>     send_prompt()   avg_miles_per_gallon    # Prompt to linear model object in R   model <- paste0(     \"Using my data, create a statistical model\",     \" investigating the relationship between two variables.\"   ) |>     answer_using_r(       objects_to_use = list(data = mtcars),       evaluate_code = TRUE,       return_mode = \"object\"     ) |>     prompt_wrap(       validation_fn = function(x) {         if (!inherits(x, \"lm\"))           return(llm_feedback(\"The output should be a linear model object.\"))         return(x)       }     ) |>     send_prompt()   summary(model)    # Prompt to plot object in R   plot <- paste0(     \"Create a scatter plot of miles per gallon (mpg) versus\",     \" horsepower (hp) for the cars in my data.\",     \" Use different colors to represent the number of cylinders (cyl).\",     \" Be very creative and make the plot look nice but also a little crazy!\"   ) |>     answer_using_r(       pkgs_to_use = c(\"ggplot2\"),       objects_to_use = list(mtcars = mtcars),       evaluate_code = TRUE,       return_mode = \"object\"     ) |>     send_prompt()   plot } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_sql.html","id":null,"dir":"Reference","previous_headings":"","what":"Enable LLM to draft and execute SQL queries on a database — answer_using_sql","title":"Enable LLM to draft and execute SQL queries on a database — answer_using_sql","text":"Enable LLM draft execute SQL queries database","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_sql.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enable LLM to draft and execute SQL queries on a database — answer_using_sql","text":"","code":"answer_using_sql(   prompt,   add_text = paste0(\"You must code in SQL to answer this prompt.\",     \" You must provide all SQL code between ```sql and ```.\", \"\\n\\n\",     \"Never make assumptions about the possible values in the tables.\\n\",     \"Instead, execute SQL queries to retrieve information you need.\"),   conn,   list_tables = TRUE,   describe_tables = TRUE,   evaluate_code = FALSE,   output_as_tool = FALSE,   return_mode = c(\"full\", \"code\", \"object\", \"formatted_output\", \"llm_answer\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_sql.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Enable LLM to draft and execute SQL queries on a database — answer_using_sql","text":"prompt single string tidyprompt() object add_text Single string added prompt text, informing LLM must use SQL answer prompt conn DBIConnection object SQL database list_tables Logical indicating whether list tables available database prompt text describe_tables Logical indicating whether describe tables available database prompt text. TRUE, columns table listed evaluate_code Logical indicating whether evaluate SQL code. TRUE, SQL code executed database results returned. Use caution, allows LLM execute arbitrary SQL code output_as_tool Logical indicating whether return output tool result. TRUE, output SQL query sent back LLM tool result. LLM can provide final answer try another query. can continue LLM provides final answer without SQL code return_mode Character string indicating return mode. Options : \"full\": Return list containing SQL code, output, formatted output \"code\": Return SQL code \"object\": Return query result object \"formatted_output\": Return formatted output: string detailing SQL code query result object.identical LLM see output output_as_tool TRUE \"llm_answer\": Return LLM answer. output tool TRUE, return mode always \"llm_answer\" (since LLM uses SQL provide final answer)","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_sql.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Enable LLM to draft and execute SQL queries on a database — answer_using_sql","text":"tidyprompt() added prompt_wrap() ensure LLM use SQL answer prompt","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_sql.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Enable LLM to draft and execute SQL queries on a database — answer_using_sql","text":"","code":"if (FALSE) { # \\dontrun{   # Create an in-memory SQLite database   conn <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")    # Create a sample table of customers   DBI::dbExecute(conn, \"   CREATE TABLE     customers (       id INTEGER PRIMARY KEY,       name TEXT,       email TEXT,       country TEXT     );   \")    # Insert some sample customer data   DBI::dbExecute(conn, \"   INSERT INTO     customers (name, email, country)   VALUES     ('Alice', 'alice@example.com', 'USA'),     ('Bob', 'bob@example.com', 'Canada'),     ('Charlie', 'charlie@example.com', 'UK'),     ('Diana', 'diana@example.com', 'USA');   \")    # Create another sample table for orders   DBI::dbExecute(conn, \"   CREATE TABLE orders (     order_id INTEGER PRIMARY KEY,     customer_id INTEGER,     product TEXT,     amount REAL,     order_date TEXT,     FOREIGN KEY(customer_id) REFERENCES customers(id)   );   \")    # Insert some sample orders   DBI::dbExecute(conn, \"   INSERT INTO     orders (customer_id, product, amount, order_date)   VALUES     (1, 'Widget', 19.99, '2024-01-15'),     (1, 'Gadget', 29.99, '2024-01-17'),     (2, 'Widget', 19.99, '2024-02-10'),     (3, 'SuperWidget', 49.99, '2024-03-05'),     (4, 'Gadget', 29.99, '2024-04-01'),     (1, 'Thingamajig', 9.99, '2024-04-02');   \")    # Ask LLM a question which it will answer using the SQL database:   \"Where are my customers from?\" |>     answer_using_sql(       conn = conn,       evaluate_code = TRUE,       output_as_tool = TRUE     ) |>     send_prompt(llm_provider_openai())   # --- Sending request to LLM provider (gpt-4o-mini): ---   # Where are my customers from?   #   # You must code in SQL to answer this prompt. You must provide all SQL code   # between ```sql and ```.   #   # Never make assumptions about the possible values in the tables.   # Instead, execute SQL queries to retrieve information you need.   #   # These tables are available in the database:   #   customers, orders   #   # Table descriptions:   #   - customers   # Columns: id, name, email, country   #   # - orders   # Columns: order_id, customer_id, product, amount, order_date   #   # Your SQL query will be executed on the database. The results will be sent back   # to you. After seeing the results, you can either provide a final answer or try   # another SQL query. When you provide your final answer, do not include any SQL code.   # --- Receiving response from LLM provider: ---   # ```sql   # SELECT DISTINCT country FROM customers;   # ```   # --- Sending request to LLM provider (gpt-4o-mini): ---   # --- SQL code: ---   # SELECT DISTINCT country FROM customers;   #   # --- Query results: ---   #   country   # 1     USA   # 2  Canada   # 3      UK   # --- Receiving response from LLM provider: ---   # Based on the query results, your customers are from the following countries:   # USA, Canada, and UK.   # [1] \"Based on the query results, your customers are from the following countries:   # USA, Canada, and UK.\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_tools.html","id":null,"dir":"Reference","previous_headings":"","what":"Enable LLM to call R functions — answer_using_tools","title":"Enable LLM to call R functions — answer_using_tools","text":"function adds ability LLM call R functions. Users can specify list functions LLM can call, prompt modified include information, well accompanying extraction function call functions (handled send_prompt()). Documentation functions extracted help file (available), documentation added tools_add_docs()","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_tools.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enable LLM to call R functions — answer_using_tools","text":"","code":"answer_using_tools(   prompt,   tools = list(),   type = c(\"text-based\", \"auto\", \"openai\", \"ollama\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_tools.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Enable LLM to call R functions — answer_using_tools","text":"prompt single string tidyprompt() object tools R function list R functions LLM can call. function documented help file (e.g., part package), documentation parsed help file. custom function, documentation added tools_add_docs() type (optional) way tool calling enabled. 'auto' automatically determine type based 'llm_provider$api_type' (note consider model compatibility, lead errors; set 'type' manually errors occur). 'openai' 'ollama' set relevant API parameters. 'text-based' provide function definitions prompt, extract function calls LLM response, call functions, providing results back via llm_feedback(). 'text-based' always works, may inefficient APIs support tool calling natively. However, 'text-based' may reliable flexible, especially combining prompt wraps. 'openai' 'ollama' may allow retries function call provide expected result. Note using 'openai' 'ollama', tool calls counted interactions may continue indefinitely (use caution)","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_tools.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Enable LLM to call R functions — answer_using_tools","text":"tidyprompt() added prompt_wrap() allow LLM call R functions","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_tools.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Enable LLM to call R functions — answer_using_tools","text":"Note method function calling purely text-based. makes suitable LLM LLM provider. However, 'native' function calling (LLM model provider restricts model special tokens can used call functions) may perform better terms accuracy efficiency. 'tidyprompt' may support 'native' function calling future","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/answer_using_tools.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Enable LLM to call R functions — answer_using_tools","text":"","code":"if (FALSE) { # \\dontrun{   # When using functions from base R or R packages,   #   documentation is automatically extracted from help files:   \"What are the files in my current directory?\" |>     answer_using_tools(dir) |> # 'dir' function is from base R     send_prompt() } # }  # Custom functions may also be provided; #   in this case, some documentation is extracted from the function's formals; #   descriptions may be added manually. See below  # Example fake weather function to add to the prompt: temperature_in_location <- function(     location = c(\"Amsterdam\", \"Utrecht\", \"Enschede\"),     unit = c(\"Celcius\", \"Fahrenheit\") ) {   location <- match.arg(location)   unit <- match.arg(unit)    temperature_celcius <- switch(     location,     \"Amsterdam\" = 32.5,     \"Utrecht\" = 19.8,     \"Enschede\" = 22.7   )    if (unit == \"Celcius\") {     return(temperature_celcius)   } else {     return(temperature_celcius * 9/5 + 32)   } }  # Generate documentation for a function #   (based on formals, & help file if available) docs <- tools_get_docs(temperature_in_location)  # The types get inferred from the function's formals # However, descriptions are still missing as the function is not from a package # We can modify the documentation object to add descriptions: docs$description <- \"Get the temperature in a location\" docs$arguments$unit$description <- \"Unit in which to return the temperature\" docs$arguments$location$description <- \"Location for which to return the temperature\" docs$return$description <- \"The temperature in the specified location and unit\" # (See `?tools_add_docs` for more details on the structure of the documentation)  # When we are satisfied with the documentation, we can add it to the function: temperature_in_location <- tools_add_docs(temperature_in_location, docs)  if (FALSE) { # \\dontrun{   # Now the LLM can use the function:   \"Hi, what is the weather in Enschede? Give me Celcius degrees\" |>     answer_using_tools(temperature_in_location) |>     send_prompt() } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.character.html","id":null,"dir":"Reference","previous_headings":"","what":"Method for chat_history() when the input is a single string — chat_history.character","title":"Method for chat_history() when the input is a single string — chat_history.character","text":"Creates chat_history object single string.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.character.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Method for chat_history() when the input is a single string — chat_history.character","text":"","code":"# S3 method for class 'character' chat_history(chat_history)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.character.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Method for chat_history() when the input is a single string — chat_history.character","text":"chat_history single string","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.data.frame.html","id":null,"dir":"Reference","previous_headings":"","what":"Method for chat_history() when the input is a data.frame — chat_history.data.frame","title":"Method for chat_history() when the input is a data.frame — chat_history.data.frame","text":"Creates chat_history object data frame.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.data.frame.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Method for chat_history() when the input is a data.frame — chat_history.data.frame","text":"","code":"# S3 method for class 'data.frame' chat_history(chat_history)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.data.frame.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Method for chat_history() when the input is a data.frame — chat_history.data.frame","text":"chat_history data frame 'role' 'content' columns, 'role' either 'user', 'assistant', 'system', 'content' character string representing chat message","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.default.html","id":null,"dir":"Reference","previous_headings":"","what":"Default method for chat_history() — chat_history.default","title":"Default method for chat_history() — chat_history.default","text":"Calls error indicates input character data.frame.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.default.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Default method for chat_history() — chat_history.default","text":"","code":"# Default S3 method chat_history(chat_history)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.default.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Default method for chat_history() — chat_history.default","text":"chat_history Object character data.frame","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.default.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Default method for chat_history() — chat_history.default","text":"input character data.frame, appropriate method called (see `chat_history.character() chat_history.data.frame()).","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.html","id":null,"dir":"Reference","previous_headings":"","what":"Create or validate chat_history object — chat_history","title":"Create or validate chat_history object — chat_history","text":"function creates validates chat_history object, ensuring matches expected format 'role' 'content' columns. separate methods data.frame character inputs includes helper function add system prompt chat history.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create or validate chat_history object — chat_history","text":"","code":"chat_history(chat_history)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create or validate chat_history object — chat_history","text":"chat_history single string, data.frame 'role' 'content' columns, NULL. data.frame provided, contain 'role' 'content' columns, 'role' either 'user', 'assistant', 'system', 'content' character string representing chat message","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create or validate chat_history object — chat_history","text":"valid chat history data.frame (class chat_history)","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/chat_history.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create or validate chat_history object — chat_history","text":"","code":"chat <- \"Hi there!\" |>   chat_history() chat #>   role   content tool_result #> 1 user Hi there!       FALSE  chat_from_df <- data.frame(   role = c(\"user\", \"assistant\"),   content = c(\"Hi there!\", \"Hello! How can I help you today?\") ) |>   chat_history() chat_from_df #>        role                          content tool_result #> 1      user                        Hi there!       FALSE #> 2 assistant Hello! How can I help you today?       FALSE  # `add_msg_to_chat_history()` may be used to add messages to a chat history chat_from_df <- chat_from_df |>   add_msg_to_chat_history(\"Calculate 2+2 for me, please!\") chat_from_df #>        role                          content tool_result #> 1      user                        Hi there!       FALSE #> 2 assistant Hello! How can I help you today?       FALSE #> 3      user    Calculate 2+2 for me, please!       FALSE  # You can also continue conversations which originate from `send_prompt()`: if (FALSE) { # \\dontrun{   result <- \"Hi there!\" |>     send_prompt(return_mode = \"full\")   # --- Sending request to LLM provider (llama3.1:8b): ---   # Hi there!   # --- Receiving response from LLM provider: ---   # It's nice to meet you. Is there something I can help you with, or would you   # like to chat?    # Access the chat history from the result:   chat_from_send_prompt <- result$chat_history    # Add a message to the chat history:   chat_history_with_new_message <- chat_from_send_prompt |>     add_msg_to_chat_history(\"Let's chat!\")    # The new chat history can be input for a new tidyprompt:   prompt <- tidyprompt(chat_history_with_new_message)    # You can also take an existing tidyprompt and add the new chat history to it;   #   this way, you can continue a conversation using the same prompt wraps   prompt$set_chat_history(chat_history_with_new_message)    # send_prompt() also accepts a chat history as input:   new_result <- chat_history_with_new_message |>     send_prompt(return_mode = \"full\")    # You can also create a persistent chat history object from   #   a chat history data frame; see ?`persistent_chat-class`   chat <- `persistent_chat-class`$new(llm_provider_ollama(), chat_from_send_prompt)   chat$chat(\"Let's chat!\") } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/construct_prompt_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Construct prompt text from a tidyprompt object — construct_prompt_text","title":"Construct prompt text from a tidyprompt object — construct_prompt_text","text":"Construct prompt text tidyprompt object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/construct_prompt_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Construct prompt text from a tidyprompt object — construct_prompt_text","text":"","code":"construct_prompt_text(x, llm_provider = NULL)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/construct_prompt_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Construct prompt text from a tidyprompt object — construct_prompt_text","text":"x tidyprompt object llm_provider optional llm_provider object. may sometimes affect prompt text construction","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/construct_prompt_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Construct prompt text from a tidyprompt object — construct_prompt_text","text":"constructed prompt text","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/construct_prompt_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Construct prompt text from a tidyprompt object — construct_prompt_text","text":"","code":"prompt <- tidyprompt(\"Hi!\") print(prompt) #> <tidyprompt> #> The base prompt is not modified by prompt wraps: #> > Hi!  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #>   # Add to a tidyprompt using a prompt wrap: prompt <- tidyprompt(\"Hi!\") |>   add_text(\"How are you?\") print(prompt) #> <tidyprompt> #> The base prompt is modified by a prompt wrap, resulting in: #> > Hi! #> >  #> > How are you?  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #> Use 'get_prompt_wraps(x)' to show the prompt wraps. #>   # Strings can be input for prompt wraps; therefore, #   a call to tidyprompt() is not necessary: prompt <- \"Hi\" |>   add_text(\"How are you?\")  # Example of adding extraction & validation with a prompt_wrap(): prompt <- \"Hi\" |>   add_text(\"What is 5 + 5?\") |>   answer_as_integer()  if (FALSE) { # \\dontrun{   # tidyprompt objects are evaluated by send_prompt(), which will   #   handle construct the prompt text, send it to the LLM provider,   #   and apply the extraction and validation functions from the tidyprompt object   prompt |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   #   Hi   #   #   What is 5 + 5?   #   #   You must answer with only an integer (use no other characters).   # --- Receiving response from LLM provider: ---   #   10   # [1] 10    # See prompt_wrap() and send_prompt() for more details } # }  # `tidyprompt` objects may be validated with these helpers: is_tidyprompt(prompt) # Returns TRUE if input is a valid tidyprompt object #> [1] TRUE  # Get base prompt text base_prompt <- prompt$base_prompt  # Get all prompt wraps prompt_wraps <- prompt$get_prompt_wraps() # Alternative: prompt_wraps <- get_prompt_wraps(prompt)  # Construct prompt text prompt_text <- prompt$construct_prompt_text() # Alternative: prompt_text <- construct_prompt_text(prompt)  # Set chat history (affecting also the base prompt) chat_history <- data.frame(   role = c(\"user\", \"assistant\", \"user\"),   content = c(\"What is 5 + 5?\", \"10\", \"And what is 5 + 6?\") ) prompt$set_chat_history(chat_history)  # Get chat history chat_history <- prompt$get_chat_history()"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/df_to_string.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a dataframe to a string representation — df_to_string","title":"Convert a dataframe to a string representation — df_to_string","text":"Converts data frame string format, intended sending LLM (display logging).","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/df_to_string.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a dataframe to a string representation — df_to_string","text":"","code":"df_to_string(df, how = c(\"wide\", \"long\"))"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/df_to_string.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a dataframe to a string representation — df_to_string","text":"df data.frame object converted string way df converted string; either \"wide\" \"long\". \"wide\" presents column names first row, followed row values new row. \"long\" presents values row together column names, repeating every row two lines whitespace","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/df_to_string.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a dataframe to a string representation — df_to_string","text":"single string representing df","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/df_to_string.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a dataframe to a string representation — df_to_string","text":"","code":"cars |>   head(5) |>   df_to_string(how = \"wide\") #> [1] \"speed, dist\\n4, 2\\n4, 10\\n7, 4\\n7, 22\\n8, 16\"  cars |>   head(5) |>   df_to_string(how = \"long\") #> [1] \"speed: 4\\ndist: 2\\n\\n\\nspeed: 4\\ndist: 10\\n\\n\\nspeed: 7\\ndist: 4\\n\\n\\nspeed: 7\\ndist: 22\\n\\n\\nspeed: 8\\ndist: 16\\n\\n\\n\""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/extract_from_return_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to extract a specific element from a list — extract_from_return_list","title":"Function to extract a specific element from a list — extract_from_return_list","text":"function intended helper function piping output send_prompt() using return_mode = \"full\". allows extract specific element list returned send_prompt(), can useful piping.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/extract_from_return_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to extract a specific element from a list — extract_from_return_list","text":"","code":"extract_from_return_list(list, name_of_element = \"response\")"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/extract_from_return_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to extract a specific element from a list — extract_from_return_list","text":"list list, typically output send_prompt() return_mode = \"full\" name_of_element character string name element extract list","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/extract_from_return_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to extract a specific element from a list — extract_from_return_list","text":"extracted element list","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/extract_from_return_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function to extract a specific element from a list — extract_from_return_list","text":"","code":"if (FALSE) { # \\dontrun{   response <- \"Hi!\" |>     send_prompt(llm_provider_ollama(), return_mode = \"full\") |>     extract_from_return_list(\"response\")   response   # [1] \"It's nice to meet you. Is there something I can help you with,   # or would you like to chat?\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/get_chat_history.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the chat history of a tidyprompt object — get_chat_history","title":"Get the chat history of a tidyprompt object — get_chat_history","text":"function gets chat history tidyprompt object. chat history constructed base prompt, system prompt, chat history field. returned object chat history system prompt first message role 'system' base prompt last message role 'user'.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/get_chat_history.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the chat history of a tidyprompt object — get_chat_history","text":"","code":"get_chat_history(x)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/get_chat_history.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the chat history of a tidyprompt object — get_chat_history","text":"x tidyprompt object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/get_chat_history.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the chat history of a tidyprompt object — get_chat_history","text":"dataframe containing chat history","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/get_chat_history.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the chat history of a tidyprompt object — get_chat_history","text":"","code":"prompt <- tidyprompt(\"Hi!\") print(prompt) #> <tidyprompt> #> The base prompt is not modified by prompt wraps: #> > Hi!  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #>   # Add to a tidyprompt using a prompt wrap: prompt <- tidyprompt(\"Hi!\") |>   add_text(\"How are you?\") print(prompt) #> <tidyprompt> #> The base prompt is modified by a prompt wrap, resulting in: #> > Hi! #> >  #> > How are you?  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #> Use 'get_prompt_wraps(x)' to show the prompt wraps. #>   # Strings can be input for prompt wraps; therefore, #   a call to tidyprompt() is not necessary: prompt <- \"Hi\" |>   add_text(\"How are you?\")  # Example of adding extraction & validation with a prompt_wrap(): prompt <- \"Hi\" |>   add_text(\"What is 5 + 5?\") |>   answer_as_integer()  if (FALSE) { # \\dontrun{   # tidyprompt objects are evaluated by send_prompt(), which will   #   handle construct the prompt text, send it to the LLM provider,   #   and apply the extraction and validation functions from the tidyprompt object   prompt |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   #   Hi   #   #   What is 5 + 5?   #   #   You must answer with only an integer (use no other characters).   # --- Receiving response from LLM provider: ---   #   10   # [1] 10    # See prompt_wrap() and send_prompt() for more details } # }  # `tidyprompt` objects may be validated with these helpers: is_tidyprompt(prompt) # Returns TRUE if input is a valid tidyprompt object #> [1] TRUE  # Get base prompt text base_prompt <- prompt$base_prompt  # Get all prompt wraps prompt_wraps <- prompt$get_prompt_wraps() # Alternative: prompt_wraps <- get_prompt_wraps(prompt)  # Construct prompt text prompt_text <- prompt$construct_prompt_text() # Alternative: prompt_text <- construct_prompt_text(prompt)  # Set chat history (affecting also the base prompt) chat_history <- data.frame(   role = c(\"user\", \"assistant\", \"user\"),   content = c(\"What is 5 + 5?\", \"10\", \"And what is 5 + 6?\") ) prompt$set_chat_history(chat_history)  # Get chat history chat_history <- prompt$get_chat_history()"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/get_prompt_wraps.html","id":null,"dir":"Reference","previous_headings":"","what":"Get prompt wraps from a tidyprompt object — get_prompt_wraps","title":"Get prompt wraps from a tidyprompt object — get_prompt_wraps","text":"Get prompt wraps tidyprompt object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/get_prompt_wraps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get prompt wraps from a tidyprompt object — get_prompt_wraps","text":"","code":"get_prompt_wraps(x, order = c(\"default\", \"modification\", \"evaluation\"))"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/get_prompt_wraps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get prompt wraps from a tidyprompt object — get_prompt_wraps","text":"x tidyprompt object order order return wraps. Options : \"default\": originally added object \"modification\": ordered modification base prompt; ordered type: check, unspecified, mode, tool, break. order prompt wraps applied construct_prompt_text() \"evaluation\": ordered evaluation LLM response; ordered type: tool, mode, break, unspecified, check. order wraps applied LLM output send_prompt()","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/get_prompt_wraps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get prompt wraps from a tidyprompt object — get_prompt_wraps","text":"list prompt wrap objects (see prompt_wrap())","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/get_prompt_wraps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get prompt wraps from a tidyprompt object — get_prompt_wraps","text":"","code":"prompt <- tidyprompt(\"Hi!\") print(prompt) #> <tidyprompt> #> The base prompt is not modified by prompt wraps: #> > Hi!  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #>   # Add to a tidyprompt using a prompt wrap: prompt <- tidyprompt(\"Hi!\") |>   add_text(\"How are you?\") print(prompt) #> <tidyprompt> #> The base prompt is modified by a prompt wrap, resulting in: #> > Hi! #> >  #> > How are you?  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #> Use 'get_prompt_wraps(x)' to show the prompt wraps. #>   # Strings can be input for prompt wraps; therefore, #   a call to tidyprompt() is not necessary: prompt <- \"Hi\" |>   add_text(\"How are you?\")  # Example of adding extraction & validation with a prompt_wrap(): prompt <- \"Hi\" |>   add_text(\"What is 5 + 5?\") |>   answer_as_integer()  if (FALSE) { # \\dontrun{   # tidyprompt objects are evaluated by send_prompt(), which will   #   handle construct the prompt text, send it to the LLM provider,   #   and apply the extraction and validation functions from the tidyprompt object   prompt |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   #   Hi   #   #   What is 5 + 5?   #   #   You must answer with only an integer (use no other characters).   # --- Receiving response from LLM provider: ---   #   10   # [1] 10    # See prompt_wrap() and send_prompt() for more details } # }  # `tidyprompt` objects may be validated with these helpers: is_tidyprompt(prompt) # Returns TRUE if input is a valid tidyprompt object #> [1] TRUE  # Get base prompt text base_prompt <- prompt$base_prompt  # Get all prompt wraps prompt_wraps <- prompt$get_prompt_wraps() # Alternative: prompt_wraps <- get_prompt_wraps(prompt)  # Construct prompt text prompt_text <- prompt$construct_prompt_text() # Alternative: prompt_text <- construct_prompt_text(prompt)  # Set chat history (affecting also the base prompt) chat_history <- data.frame(   role = c(\"user\", \"assistant\", \"user\"),   content = c(\"What is 5 + 5?\", \"10\", \"And what is 5 + 6?\") ) prompt$set_chat_history(chat_history)  # Get chat history chat_history <- prompt$get_chat_history()"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/is_tidyprompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if object is a tidyprompt object — is_tidyprompt","title":"Check if object is a tidyprompt object — is_tidyprompt","text":"Check object tidyprompt object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/is_tidyprompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if object is a tidyprompt object — is_tidyprompt","text":"","code":"is_tidyprompt(x)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/is_tidyprompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if object is a tidyprompt object — is_tidyprompt","text":"x object check","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/is_tidyprompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if object is a tidyprompt object — is_tidyprompt","text":"TRUE object valid tidyprompt object, otherwise FALSE","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/is_tidyprompt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if object is a tidyprompt object — is_tidyprompt","text":"","code":"prompt <- tidyprompt(\"Hi!\") print(prompt) #> <tidyprompt> #> The base prompt is not modified by prompt wraps: #> > Hi!  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #>   # Add to a tidyprompt using a prompt wrap: prompt <- tidyprompt(\"Hi!\") |>   add_text(\"How are you?\") print(prompt) #> <tidyprompt> #> The base prompt is modified by a prompt wrap, resulting in: #> > Hi! #> >  #> > How are you?  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #> Use 'get_prompt_wraps(x)' to show the prompt wraps. #>   # Strings can be input for prompt wraps; therefore, #   a call to tidyprompt() is not necessary: prompt <- \"Hi\" |>   add_text(\"How are you?\")  # Example of adding extraction & validation with a prompt_wrap(): prompt <- \"Hi\" |>   add_text(\"What is 5 + 5?\") |>   answer_as_integer()  if (FALSE) { # \\dontrun{   # tidyprompt objects are evaluated by send_prompt(), which will   #   handle construct the prompt text, send it to the LLM provider,   #   and apply the extraction and validation functions from the tidyprompt object   prompt |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   #   Hi   #   #   What is 5 + 5?   #   #   You must answer with only an integer (use no other characters).   # --- Receiving response from LLM provider: ---   #   10   # [1] 10    # See prompt_wrap() and send_prompt() for more details } # }  # `tidyprompt` objects may be validated with these helpers: is_tidyprompt(prompt) # Returns TRUE if input is a valid tidyprompt object #> [1] TRUE  # Get base prompt text base_prompt <- prompt$base_prompt  # Get all prompt wraps prompt_wraps <- prompt$get_prompt_wraps() # Alternative: prompt_wraps <- get_prompt_wraps(prompt)  # Construct prompt text prompt_text <- prompt$construct_prompt_text() # Alternative: prompt_text <- construct_prompt_text(prompt)  # Set chat history (affecting also the base prompt) chat_history <- data.frame(   role = c(\"user\", \"assistant\", \"user\"),   content = c(\"What is 5 + 5?\", \"10\", \"And what is 5 + 6?\") ) prompt$set_chat_history(chat_history)  # Get chat history chat_history <- prompt$get_chat_history()"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_break.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an llm_break object — llm_break","title":"Create an llm_break object — llm_break","text":"object used break extraction validation loop defined prompt_wrap() evaluated send_prompt(). extraction validation function returns object, loop broken extraction validation functions applied; instead, send_prompt() able return result point. may useful scenarios determined LLM unable provide response prompt.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_break.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an llm_break object — llm_break","text":"","code":"llm_break(object_to_return = NULL, success = FALSE)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_break.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an llm_break object — llm_break","text":"object_to_return object return response result send_prompt() object returned extraction validation function. success logical indicating whether send_prompt() loop break nonetheless considered successful completion extraction validation process. FALSE, object_to_return must NULL (response result send_prompt() always 'NULL' evaluation unsuccessful); FALSE, send_prompt() also print warning unsuccessful evaluation. TRUE, object_to_return returned response result send_prompt() (send_prompt()) print warning unsuccessful evaluation).","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_break.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an llm_break object — llm_break","text":"list class \"llm_break\" containing object return logical indicating whether evaluation successful","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_break.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an llm_break object — llm_break","text":"","code":"# Example usage within an extraction function similar to the one in 'quit_if()': extraction_fn <- function(x) {   quit_detect_regex <- \"NO ANSWER\"    if (grepl(quit_detect_regex, x)) {       return(llm_break(         object_to_return = NULL,         success = TRUE       ))   }    return(x) }  if (FALSE) { # \\dontrun{   result <- \"How many months old is the cat of my uncle?\" |>     answer_as_integer() |>     prompt_wrap(       modify_fn = function(prompt) {         paste0(           prompt, \"\\n\\n\",           \"Type only 'NO ANSWER' if you do not know.\"         )       },       extraction_fn = extraction_fn,       type = \"break\"     ) |>     send_prompt()   result   # NULL } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_feedback.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an llm_feedback object — llm_feedback","title":"Create an llm_feedback object — llm_feedback","text":"object used send feedback LLM LLM reply succesfully pass extraction validation function (handled send_prompt() defined using prompt_wrap()). feedback text sent back LLM. extraction validation function return object feedback text sent LLM.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_feedback.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an llm_feedback object — llm_feedback","text":"","code":"llm_feedback(text, tool_result = FALSE)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_feedback.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an llm_feedback object — llm_feedback","text":"text character string containing feedback text. sent back LLM passing extractor validator function tool_result logical indicating whether feedback tool result. TRUE, send_prompt() remove chat history cleaning context window repeated interactions","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_feedback.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an llm_feedback object — llm_feedback","text":"object class \"llm_feedback\" (\"llm_feedback_tool_result\") containing feedback text send back LLM","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_feedback.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an llm_feedback object — llm_feedback","text":"","code":"# Example usage within a validation function similar to the one in 'answer_as_integer()': validation_fn <- function(x, min = 0, max = 100) {   if (x != floor(x)) { # Not a whole number     return(llm_feedback(       \"You must answer with only an integer (use no other characters).\"     ))   }   if (!is.null(min) && x < min) {     return(llm_feedback(glue::glue(       \"The number should be greater than or equal to {min}.\"     )))   }   if (!is.null(max) && x > max) {     return(llm_feedback(glue::glue(       \"The number should be less than or equal to {max}.\"     )))   }   return(TRUE) }  # This validation_fn would be part of a prompt_wrap(); #   see the `answer_as_integer()` function for an example of how to use it"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":null,"dir":"Reference","previous_headings":"","what":"LlmProvider R6 Class — llm_provider-class","title":"LlmProvider R6 Class — llm_provider-class","text":"class provides structure creating llm_provider objects different implementations $complete_chat(). Using class, can create llm_provider object interacts different LLM providers, Ollama, OpenAI, custom providers.","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"LlmProvider R6 Class — llm_provider-class","text":"parameters named list parameters configure llm_provider. Parameters may appended request body interacting LLM provider API verbose logical indicating whether interaction LLM provider printed console url URL LLM provider API endpoint chat completion api_key API key use authentication LLM provider API api_type type API use (e.g., \"openai\", \"ollama\"). used determine certain specific behaviors different APIs, instance, done answer_as_json() function handler_fns list functions called completion chat. See $add_handler_fn()","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"LlmProvider R6 Class — llm_provider-class","text":"llm_provider-class$new() llm_provider-class$set_parameters() llm_provider-class$complete_chat() llm_provider-class$add_handler_fn() llm_provider-class$set_handler_fns() llm_provider-class$clone()","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"LlmProvider R6 Class — llm_provider-class","text":"Create new llm_provider object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"LlmProvider R6 Class — llm_provider-class","text":"","code":"llm_provider-class$new(   complete_chat_function,   parameters = list(),   verbose = TRUE,   url = NULL,   api_key = NULL,   api_type = \"unspecified\" )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"LlmProvider R6 Class — llm_provider-class","text":"complete_chat_function Function called llm_provider complete chat. function take list containing least '$chat_history' (data frame 'role' 'content' columns) return response object, contains: 'completed': dataframe 'role' 'content' columns, containing completed chat history 'http': list containing list 'requests' list 'responses', containing HTTP requests responses made chat completion parameters named list parameters configure llm_provider. parameters may appended request body interacting LLM provider. example, model parameter may often required. 'stream' parameter may used indicate API stream. Parameters include chat_history, 'api_key' 'url', handled separately llm_provider '$complete_chat()'. Parameters also set handled prompt wraps verbose logical indicating whether interaction LLM provider printed console url URL LLM provider API endpoint chat completion (typically required, may left NULL cases, instance creating fake LLM provider) api_key API key use authentication LLM provider API (optional, required , instance, Ollama) api_type type API use (e.g., \"openai\", \"ollama\"). used determine certain specific behaviors different APIs (see example answer_as_json() function)","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"LlmProvider R6 Class — llm_provider-class","text":"new llm_provider R6 object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"method-set-parameters-","dir":"Reference","previous_headings":"","what":"Method set_parameters()","title":"LlmProvider R6 Class — llm_provider-class","text":"Helper function set parameters llm_provider object. function appends new parameters existing parameters list.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"LlmProvider R6 Class — llm_provider-class","text":"","code":"llm_provider-class$set_parameters(new_parameters)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"LlmProvider R6 Class — llm_provider-class","text":"new_parameters named list new parameters append existing parameters list","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"LlmProvider R6 Class — llm_provider-class","text":"modified llm_provider object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"method-complete-chat-","dir":"Reference","previous_headings":"","what":"Method complete_chat()","title":"LlmProvider R6 Class — llm_provider-class","text":"Sends chat history (see chat_history() details) LLM provider using configured $complete_chat(). function typically called send_prompt() interact LLM provider, can also called directly.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"LlmProvider R6 Class — llm_provider-class","text":"","code":"llm_provider-class$complete_chat(input)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"LlmProvider R6 Class — llm_provider-class","text":"input string, data frame valid chat history (see chat_history()), list containing valid chat history key '$chat_history'","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"LlmProvider R6 Class — llm_provider-class","text":"response LLM provider","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"method-add-handler-fn-","dir":"Reference","previous_headings":"","what":"Method add_handler_fn()","title":"LlmProvider R6 Class — llm_provider-class","text":"Helper function add handler function llm_provider object. Handler functions called completion chat can used modify response returned llm_provider. handler function take response object input (first argument) well 'self' (llm_provider object) return modified response object. functions called order added list.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"LlmProvider R6 Class — llm_provider-class","text":"","code":"llm_provider-class$add_handler_fn(handler_fn)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"LlmProvider R6 Class — llm_provider-class","text":"handler_fn function takes response object plus 'self' (llm_provider object) input returns modified response object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"LlmProvider R6 Class — llm_provider-class","text":"handler function returns list 'break' field set TRUE, chat completion interrupted response returned point. handler function returns list 'done' field set FALSE, handler functions continue called loop 'done' field set FALSE.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"method-set-handler-fns-","dir":"Reference","previous_headings":"","what":"Method set_handler_fns()","title":"LlmProvider R6 Class — llm_provider-class","text":"Helper function set handler functions llm_provider object. function replaces existing handler functions list new list handler functions. See $add_handler_fn() information","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"LlmProvider R6 Class — llm_provider-class","text":"","code":"llm_provider-class$set_handler_fns(handler_fns)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"LlmProvider R6 Class — llm_provider-class","text":"handler_fns list handler functions set","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"LlmProvider R6 Class — llm_provider-class","text":"objects class cloneable method.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"LlmProvider R6 Class — llm_provider-class","text":"","code":"llm_provider-class$clone(deep = FALSE)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"LlmProvider R6 Class — llm_provider-class","text":"deep Whether make deep clone.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider-class.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"LlmProvider R6 Class — llm_provider-class","text":"","code":"# Example creation of a llm_provider-class object: llm_provider_openai <- function(     parameters = list(       model = \"gpt-4o-mini\",       stream = getOption(\"tidyprompt.stream\", TRUE)     ),     verbose = getOption(\"tidyprompt.verbose\", TRUE),     url = \"https://api.openai.com/v1/chat/completions\",     api_key = Sys.getenv(\"OPENAI_API_KEY\") ) {   complete_chat <- function(chat_history) {     headers <- c(       \"Content-Type\" = \"application/json\",       \"Authorization\" = paste(\"Bearer\", self$api_key)     )      body <- list(       messages = lapply(seq_len(nrow(chat_history)), function(i) {         list(role = chat_history$role[i], content = chat_history$content[i])       })     )      for (name in names(self$parameters))       body[[name]] <- self$parameters[[name]]      request <- httr2::request(self$url) |>       httr2::req_body_json(body) |>       httr2::req_headers(!!!headers)      request_llm_provider(       chat_history,       request,       stream = self$parameters$stream,       verbose = self$verbose,       api_type = self$api_type     )   }    return(`llm_provider-class`$new(     complete_chat_function = complete_chat,     parameters = parameters,     verbose = verbose,     url = url,     api_key = api_key,     api_type = \"openai\"   )) }  llm_provider <- llm_provider_openai()  if (FALSE) { # \\dontrun{   llm_provider$complete_chat(\"Hi!\")   # --- Sending request to LLM provider (gpt-4o-mini): ---   # Hi!   # --- Receiving response from LLM provider: ---   # Hello! How can I assist you today? } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_google_gemini.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new Google Gemini LLM provider — llm_provider_google_gemini","title":"Create a new Google Gemini LLM provider — llm_provider_google_gemini","text":"function creates new llm_provider object interacts Google Gemini API. Streaming yet supported implementation.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_google_gemini.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new Google Gemini LLM provider — llm_provider_google_gemini","text":"","code":"llm_provider_google_gemini(   parameters = list(model = \"gemini-1.5-flash\"),   verbose = getOption(\"tidyprompt.verbose\", TRUE),   url = \"https://generativelanguage.googleapis.com/v1beta/models/\",   api_key = Sys.getenv(\"GOOGLE_AI_STUDIO_API_KEY\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_google_gemini.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new Google Gemini LLM provider — llm_provider_google_gemini","text":"parameters named list parameters. Currently following parameters required: model: name model use (see: https://ai.google.dev/gemini-api/docs/models/gemini) Additional parameters appended request body; see Google AI Studio API documentation information: https://ai.google.dev/gemini-api/docs/text-generation & https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/get-started/rest.ipynb verbose logical indicating whether interaction LLM provider printed console url URL Google Gemini API endpoint chat completion api_key API key use authentication Google Gemini API (see: https://aistudio.google.com/app/apikey)","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_google_gemini.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a new Google Gemini LLM provider — llm_provider_google_gemini","text":"new llm_provider object use Google Gemini API new llm_provider object use Google Gemini API","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_google_gemini.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a new Google Gemini LLM provider — llm_provider_google_gemini","text":"","code":"# Various providers: ollama <- llm_provider_ollama() openai <- llm_provider_openai() openrouter <- llm_provider_openrouter() mistral <- llm_provider_mistral() groq <- llm_provider_groq() xai <- llm_provider_xai() gemini <- llm_provider_google_gemini()  # Initialize with settings: ollama <- llm_provider_ollama(   parameters = list(     model = \"llama3.2:3b\",     stream = TRUE   ),   verbose = TRUE,   url = \"http://localhost:11434/api/chat\" )  # Change settings: ollama$verbose <- FALSE ollama$parameters$stream <- FALSE ollama$parameters$model <- \"llama3.1:8b\"  if (FALSE) { # \\dontrun{   # Try a simple chat message with '$complete_chat()':   response <- ollama$complete_chat(\"Hi!\")   response   # $role   # [1] \"assistant\"   #   # $content   # [1] \"How's it going? Is there something I can help you with or would you like   # to chat?\"   #   # $http   # Response [http://localhost:11434/api/chat]   # Date: 2024-11-18 14:21   # Status: 200   # Content-Type: application/json; charset=utf-8   # Size: 375 B    # Use with send_prompt():   \"Hi\" |>     send_prompt(ollama)   # [1] \"How's your day going so far? Is there something I can help you with or   # would you like to chat?\" } # } # Various providers: ollama <- llm_provider_ollama() openai <- llm_provider_openai() openrouter <- llm_provider_openrouter() mistral <- llm_provider_mistral() groq <- llm_provider_groq() xai <- llm_provider_xai() gemini <- llm_provider_google_gemini()  # Initialize with settings: ollama <- llm_provider_ollama(   parameters = list(     model = \"llama3.2:3b\",     stream = TRUE   ),   verbose = TRUE,   url = \"http://localhost:11434/api/chat\" )  # Change settings: ollama$verbose <- FALSE ollama$parameters$stream <- FALSE ollama$parameters$model <- \"llama3.1:8b\"  if (FALSE) { # \\dontrun{   # Try a simple chat message with '$complete_chat()':   response <- ollama$complete_chat(\"Hi!\")   response   # $role   # [1] \"assistant\"   #   # $content   # [1] \"How's it going? Is there something I can help you with or would you like   # to chat?\"   #   # $http   # Response [http://localhost:11434/api/chat]   # Date: 2024-11-18 14:21   # Status: 200   # Content-Type: application/json; charset=utf-8   # Size: 375 B    # Use with send_prompt():   \"Hi\" |>     send_prompt(ollama)   # [1] \"How's your day going so far? Is there something I can help you with or   # would you like to chat?\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_groq.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new Groq LLM provider — llm_provider_groq","title":"Create a new Groq LLM provider — llm_provider_groq","text":"function creates new llm_provider object interacts Groq API.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_groq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new Groq LLM provider — llm_provider_groq","text":"","code":"llm_provider_groq(   parameters = list(model = \"llama-3.1-8b-instant\", stream = TRUE),   verbose = getOption(\"tidyprompt.verbose\", TRUE),   url = \"https://api.groq.com/openai/v1/chat/completions\",   api_key = Sys.getenv(\"GROQ_API_KEY\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_groq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new Groq LLM provider — llm_provider_groq","text":"parameters named list parameters. Currently following parameters required: model: name model use stream: logical indicating whether API stream responses Additional parameters appended request body; see Groq API documentation information: https://console.groq.com/docs/api-reference#chat-create verbose logical indicating whether interaction LLM provider printed console url URL Groq API endpoint chat completion api_key API key use authentication Groq API","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_groq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a new Groq LLM provider — llm_provider_groq","text":"new llm_provider object use Groq API","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_groq.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a new Groq LLM provider — llm_provider_groq","text":"","code":"# Various providers: ollama <- llm_provider_ollama() openai <- llm_provider_openai() openrouter <- llm_provider_openrouter() mistral <- llm_provider_mistral() groq <- llm_provider_groq() xai <- llm_provider_xai() gemini <- llm_provider_google_gemini()  # Initialize with settings: ollama <- llm_provider_ollama(   parameters = list(     model = \"llama3.2:3b\",     stream = TRUE   ),   verbose = TRUE,   url = \"http://localhost:11434/api/chat\" )  # Change settings: ollama$verbose <- FALSE ollama$parameters$stream <- FALSE ollama$parameters$model <- \"llama3.1:8b\"  if (FALSE) { # \\dontrun{   # Try a simple chat message with '$complete_chat()':   response <- ollama$complete_chat(\"Hi!\")   response   # $role   # [1] \"assistant\"   #   # $content   # [1] \"How's it going? Is there something I can help you with or would you like   # to chat?\"   #   # $http   # Response [http://localhost:11434/api/chat]   # Date: 2024-11-18 14:21   # Status: 200   # Content-Type: application/json; charset=utf-8   # Size: 375 B    # Use with send_prompt():   \"Hi\" |>     send_prompt(ollama)   # [1] \"How's your day going so far? Is there something I can help you with or   # would you like to chat?\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_mistral.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new Mistral LLM provider — llm_provider_mistral","title":"Create a new Mistral LLM provider — llm_provider_mistral","text":"function creates new llm_provider object interacts Mistral API.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_mistral.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new Mistral LLM provider — llm_provider_mistral","text":"","code":"llm_provider_mistral(   parameters = list(model = \"ministral-3b-latest\", stream =     getOption(\"tidyprompt.stream\", TRUE)),   verbose = getOption(\"tidyprompt.verbose\", TRUE),   url = \"https://api.mistral.ai/v1/chat/completions\",   api_key = Sys.getenv(\"MISTRAL_API_KEY\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_mistral.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new Mistral LLM provider — llm_provider_mistral","text":"parameters named list parameters. Currently following parameters required: model: name model use stream: logical indicating whether API stream responses Additional parameters appended request body; see Mistral API documentation information: https://docs.mistral.ai/api/#tag/chat verbose logical indicating whether interaction LLM provider printed consol url URL Mistral API endpoint chat completion api_key API key use authentication Mistral API","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_mistral.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a new Mistral LLM provider — llm_provider_mistral","text":"new llm_provider object use Mistral API","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_mistral.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a new Mistral LLM provider — llm_provider_mistral","text":"","code":"# Various providers: ollama <- llm_provider_ollama() openai <- llm_provider_openai() openrouter <- llm_provider_openrouter() mistral <- llm_provider_mistral() groq <- llm_provider_groq() xai <- llm_provider_xai() gemini <- llm_provider_google_gemini()  # Initialize with settings: ollama <- llm_provider_ollama(   parameters = list(     model = \"llama3.2:3b\",     stream = TRUE   ),   verbose = TRUE,   url = \"http://localhost:11434/api/chat\" )  # Change settings: ollama$verbose <- FALSE ollama$parameters$stream <- FALSE ollama$parameters$model <- \"llama3.1:8b\"  if (FALSE) { # \\dontrun{   # Try a simple chat message with '$complete_chat()':   response <- ollama$complete_chat(\"Hi!\")   response   # $role   # [1] \"assistant\"   #   # $content   # [1] \"How's it going? Is there something I can help you with or would you like   # to chat?\"   #   # $http   # Response [http://localhost:11434/api/chat]   # Date: 2024-11-18 14:21   # Status: 200   # Content-Type: application/json; charset=utf-8   # Size: 375 B    # Use with send_prompt():   \"Hi\" |>     send_prompt(ollama)   # [1] \"How's your day going so far? Is there something I can help you with or   # would you like to chat?\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_ollama.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new Ollama LLM provider — llm_provider_ollama","title":"Create a new Ollama LLM provider — llm_provider_ollama","text":"function creates new llm_provider object interacts Ollama API.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_ollama.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new Ollama LLM provider — llm_provider_ollama","text":"","code":"llm_provider_ollama(   parameters = list(model = \"llama3.1:8b\", stream = getOption(\"tidyprompt.stream\", TRUE)),   verbose = getOption(\"tidyprompt.verbose\", TRUE),   url = \"http://localhost:11434/api/chat\" )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_ollama.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new Ollama LLM provider — llm_provider_ollama","text":"parameters named list parameters. Currently following parameters required: model: name model use stream: logical indicating whether API stream responses Additional parameters may passed adding parameters list; parameters passed Ollama API via body POST request. Note various Ollama options need set list named 'options' within parameters list (e.g., context window size represented $parameters$options$num_ctx) verbose logical indicating whether interaction LLM provider printed console url URL Ollama API endpoint chat completion (typically: \"http://localhost:11434/api/chat\")","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_ollama.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a new Ollama LLM provider — llm_provider_ollama","text":"new llm_provider object use Ollama API","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_ollama.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a new Ollama LLM provider — llm_provider_ollama","text":"","code":"# Various providers: ollama <- llm_provider_ollama() openai <- llm_provider_openai() openrouter <- llm_provider_openrouter() mistral <- llm_provider_mistral() groq <- llm_provider_groq() xai <- llm_provider_xai() gemini <- llm_provider_google_gemini()  # Initialize with settings: ollama <- llm_provider_ollama(   parameters = list(     model = \"llama3.2:3b\",     stream = TRUE   ),   verbose = TRUE,   url = \"http://localhost:11434/api/chat\" )  # Change settings: ollama$verbose <- FALSE ollama$parameters$stream <- FALSE ollama$parameters$model <- \"llama3.1:8b\"  if (FALSE) { # \\dontrun{   # Try a simple chat message with '$complete_chat()':   response <- ollama$complete_chat(\"Hi!\")   response   # $role   # [1] \"assistant\"   #   # $content   # [1] \"How's it going? Is there something I can help you with or would you like   # to chat?\"   #   # $http   # Response [http://localhost:11434/api/chat]   # Date: 2024-11-18 14:21   # Status: 200   # Content-Type: application/json; charset=utf-8   # Size: 375 B    # Use with send_prompt():   \"Hi\" |>     send_prompt(ollama)   # [1] \"How's your day going so far? Is there something I can help you with or   # would you like to chat?\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new OpenAI LLM provider — llm_provider_openai","title":"Create a new OpenAI LLM provider — llm_provider_openai","text":"function creates new llm_provider object interacts Open AI API.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new OpenAI LLM provider — llm_provider_openai","text":"","code":"llm_provider_openai(   parameters = list(model = \"gpt-4o-mini\", stream = getOption(\"tidyprompt.stream\", TRUE)),   verbose = getOption(\"tidyprompt.verbose\", TRUE),   url = \"https://api.openai.com/v1/chat/completions\",   api_key = Sys.getenv(\"OPENAI_API_KEY\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new OpenAI LLM provider — llm_provider_openai","text":"parameters named list parameters. Currently following parameters required: model: name model use api_key: API key use authentication OpenAI API. project API key (user API key) url: URL OpenAI API (may also alternative endpoint provides similar API.) stream: logical indicating whether API stream responses Additional parameters appended request body; see OpenAI API documentation information: https://platform.openai.com/docs/api-reference/chat verbose logical indicating whether interaction LLM provider printed console. Default TRUE. url URL OpenAI API endpoint chat completion (typically: \"https://api.openai.com/v1/chat/completions\") api_key API key use authentication OpenAI API","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a new OpenAI LLM provider — llm_provider_openai","text":"new llm_provider object use OpenAI API","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a new OpenAI LLM provider — llm_provider_openai","text":"","code":"# Various providers: ollama <- llm_provider_ollama() openai <- llm_provider_openai() openrouter <- llm_provider_openrouter() mistral <- llm_provider_mistral() groq <- llm_provider_groq() xai <- llm_provider_xai() gemini <- llm_provider_google_gemini()  # Initialize with settings: ollama <- llm_provider_ollama(   parameters = list(     model = \"llama3.2:3b\",     stream = TRUE   ),   verbose = TRUE,   url = \"http://localhost:11434/api/chat\" )  # Change settings: ollama$verbose <- FALSE ollama$parameters$stream <- FALSE ollama$parameters$model <- \"llama3.1:8b\"  if (FALSE) { # \\dontrun{   # Try a simple chat message with '$complete_chat()':   response <- ollama$complete_chat(\"Hi!\")   response   # $role   # [1] \"assistant\"   #   # $content   # [1] \"How's it going? Is there something I can help you with or would you like   # to chat?\"   #   # $http   # Response [http://localhost:11434/api/chat]   # Date: 2024-11-18 14:21   # Status: 200   # Content-Type: application/json; charset=utf-8   # Size: 375 B    # Use with send_prompt():   \"Hi\" |>     send_prompt(ollama)   # [1] \"How's your day going so far? Is there something I can help you with or   # would you like to chat?\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_openrouter.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new OpenRouter LLM provider — llm_provider_openrouter","title":"Create a new OpenRouter LLM provider — llm_provider_openrouter","text":"function creates new llm_provider object interacts OpenRouter API.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_openrouter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new OpenRouter LLM provider — llm_provider_openrouter","text":"","code":"llm_provider_openrouter(   parameters = list(model = \"qwen/qwen-2.5-7b-instruct\", stream =     getOption(\"tidyprompt.stream\", TRUE)),   verbose = getOption(\"tidyprompt.verbose\", TRUE),   url = \"https://openrouter.ai/api/v1/chat/completions\",   api_key = Sys.getenv(\"OPENROUTER_API_KEY\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_openrouter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new OpenRouter LLM provider — llm_provider_openrouter","text":"parameters named list parameters. Currently following parameters required: model: name model use stream: logical indicating whether API stream responses Additional parameters appended request body; see OpenRouter API documentation information: https://openrouter.ai/docs/parameters verbose logical indicating whether interaction LLM provider printed console. url URL OpenRouter API endpoint chat completion api_key API key use authentication OpenRouter API","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_openrouter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a new OpenRouter LLM provider — llm_provider_openrouter","text":"new llm_provider object use OpenRouter API","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_openrouter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a new OpenRouter LLM provider — llm_provider_openrouter","text":"","code":"# Various providers: ollama <- llm_provider_ollama() openai <- llm_provider_openai() openrouter <- llm_provider_openrouter() mistral <- llm_provider_mistral() groq <- llm_provider_groq() xai <- llm_provider_xai() gemini <- llm_provider_google_gemini()  # Initialize with settings: ollama <- llm_provider_ollama(   parameters = list(     model = \"llama3.2:3b\",     stream = TRUE   ),   verbose = TRUE,   url = \"http://localhost:11434/api/chat\" )  # Change settings: ollama$verbose <- FALSE ollama$parameters$stream <- FALSE ollama$parameters$model <- \"llama3.1:8b\"  if (FALSE) { # \\dontrun{   # Try a simple chat message with '$complete_chat()':   response <- ollama$complete_chat(\"Hi!\")   response   # $role   # [1] \"assistant\"   #   # $content   # [1] \"How's it going? Is there something I can help you with or would you like   # to chat?\"   #   # $http   # Response [http://localhost:11434/api/chat]   # Date: 2024-11-18 14:21   # Status: 200   # Content-Type: application/json; charset=utf-8   # Size: 375 B    # Use with send_prompt():   \"Hi\" |>     send_prompt(ollama)   # [1] \"How's your day going so far? Is there something I can help you with or   # would you like to chat?\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_xai.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new XAI (Grok) LLM provider — llm_provider_xai","title":"Create a new XAI (Grok) LLM provider — llm_provider_xai","text":"function creates new llm_provider object interacts XAI API.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_xai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new XAI (Grok) LLM provider — llm_provider_xai","text":"","code":"llm_provider_xai(   parameters = list(model = \"grok-beta\", stream = getOption(\"tidyprompt.stream\", TRUE)),   verbose = getOption(\"tidyprompt.verbose\", TRUE),   url = \"https://api.x.ai/v1/chat/completions\",   api_key = Sys.getenv(\"XAI_API_KEY\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_xai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new XAI (Grok) LLM provider — llm_provider_xai","text":"parameters named list parameters. Currently following parameters required: model: name model use stream: logical indicating whether API stream responses Additional parameters appended request body; see XAI API documentation information: https://docs.x.ai/api/endpoints#chat-completions verbose logical indicating whether interaction LLM provider printed console. Default TRUE. url URL XAI API endpoint chat completion api_key API key use authentication XAI API","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_xai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a new XAI (Grok) LLM provider — llm_provider_xai","text":"new llm_provider object use XAI API","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_provider_xai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a new XAI (Grok) LLM provider — llm_provider_xai","text":"","code":"# Various providers: ollama <- llm_provider_ollama() openai <- llm_provider_openai() openrouter <- llm_provider_openrouter() mistral <- llm_provider_mistral() groq <- llm_provider_groq() xai <- llm_provider_xai() gemini <- llm_provider_google_gemini()  # Initialize with settings: ollama <- llm_provider_ollama(   parameters = list(     model = \"llama3.2:3b\",     stream = TRUE   ),   verbose = TRUE,   url = \"http://localhost:11434/api/chat\" )  # Change settings: ollama$verbose <- FALSE ollama$parameters$stream <- FALSE ollama$parameters$model <- \"llama3.1:8b\"  if (FALSE) { # \\dontrun{   # Try a simple chat message with '$complete_chat()':   response <- ollama$complete_chat(\"Hi!\")   response   # $role   # [1] \"assistant\"   #   # $content   # [1] \"How's it going? Is there something I can help you with or would you like   # to chat?\"   #   # $http   # Response [http://localhost:11434/api/chat]   # Date: 2024-11-18 14:21   # Status: 200   # Content-Type: application/json; charset=utf-8   # Size: 375 B    # Use with send_prompt():   \"Hi\" |>     send_prompt(ollama)   # [1] \"How's your day going so far? Is there something I can help you with or   # would you like to chat?\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_verify.html","id":null,"dir":"Reference","previous_headings":"","what":"Have LLM check the result of a prompt (LLM-in-the-loop) — llm_verify","title":"Have LLM check the result of a prompt (LLM-in-the-loop) — llm_verify","text":"function wrap prompt check LLM accept decline result prompt, providing feedback result declined. evaluating LLM presented original prompt result prompt, asked verify answer satisfactory (using chain thought reasoning arrive boolean decision). result declined, chain thought responsible decision summarized sent back original LLM asked evaluate prompt, may retry prompt. Note function experimental , relies chain thought reasoning LLM answer another LLM, may always provide accurate results can increase token cost evaluating prompt.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_verify.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Have LLM check the result of a prompt (LLM-in-the-loop) — llm_verify","text":"","code":"llm_verify(   prompt,   question = \"Is the answer satisfactory?\",   llm_provider = NULL,   max_words_feedback = 50 )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_verify.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Have LLM check the result of a prompt (LLM-in-the-loop) — llm_verify","text":"prompt single string tidyprompt object question question ask LLM verify result prompt. LLM presented original prompt, result, question. LLM asked provide boolean answer question. TRUE, result prompt accepted; FALSE, result declined llm_provider llm_provider object used verify evaluation led satisfactory result. provided, LLM provider prompt originally evaluated used max_words_feedback maximum number words allowed summary result declined. summary sent back LLM originally asked evaluate prompt","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_verify.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Have LLM check the result of a prompt (LLM-in-the-loop) — llm_verify","text":"tidyprompt added prompt_wrap() add check LLM accept decline result prompt, providing feedback result declined","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_verify.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Have LLM check the result of a prompt (LLM-in-the-loop) — llm_verify","text":"original prompt text shown LLM built base prompt well prompt wraps modify function extraction validation function. ensure redundant validation performed evaluating LLM instructions already validated functions prompt wraps.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/llm_verify.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Have LLM check the result of a prompt (LLM-in-the-loop) — llm_verify","text":"","code":"if (FALSE) { # \\dontrun{   \"What is 'Enschede'?!\" |>     answer_as_text(max_words = 50) |>     llm_verify() |>     send_prompt() #   --- Sending request to LLM provider (gpt-4o-mini): --- #   What is 'Enschede'?! # #   You must provide a text response. The response must be at most 50 words. #   --- Receiving response from LLM provider: --- #   Enschede is a city in the Netherlands, located in the eastern part near the German border. #   It is known for its vibrant culture, history, and universities, particularly the #   University of Twente, as well as its textiles industry and beautiful parks. #   --- Sending request to LLM provider (gpt-4o-mini): --- #   You are given a user's prompt. #   To answer the user's prompt, you need to think step by step to arrive at a final answer. # #   ----- START OF USER'S PROMPT ----- #   >>> An assistant was asked: # #   What is 'Enschede'?! # #   >>> The assistant answered: # #   [1] \"Enschede is a city in the Netherlands, located in the eastern part near #   the German border. It is known for its vibrant culture, history, and universities, #   particularly the University of Twente, as well as its textiles industry and #   beautiful parks.\" # #   >>> Is the answer satisfactory? # #   You must answer with only TRUE or FALSE (use no other characters). #   ----- END OF USER'S PROMPT ----- # #   What are the steps you would take to answer the user's prompt? #   Describe your thought process in the following format: #     >> step 1: <step 1 description> #     >> step 2: <step 2 description> #     (etc.) # #   When you are done, you must type: #     FINISH[<put here your final answer to the user's prompt>] # #   Make sure your final answer follows the logical conclusion of your thought process. #   --- Receiving response from LLM provider: --- #   >> step 1: Read the user's prompt carefully to understand what is being asked #   regarding the assistant's answer about Enschede. #   >> step 2: Analyze the assistant's provided answer for accuracy and completeness, #   ensuring it conveys relevant information about Enschede. #   >> step 3: Assess whether the answer includes key aspects, such as the city's #   location, cultural significance, educational institutions, and any notable #   historical references or industries tied to the city. #   >> step 4: Determine if the assistant's answer sufficiently addresses the #   question \"What is Enschede?\" and provides a clear and informative overview of the city. #   >> step 5: Conclude whether the assistant's answer meets the standards for being #   satisfactory in terms of detail and correctness. # #   FINISH[TRUE] #   [1] \"Enschede is a city in the Netherlands, located in the eastern part near the #   German border. It is known for its vibrant culture, history, and universities, #   particularly the University of Twente, as well as its textiles industry and beautiful parks.\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":null,"dir":"Reference","previous_headings":"","what":"PersistentChat R6 class — persistent_chat-class","title":"PersistentChat R6 class — persistent_chat-class","text":"class managing persistent chat large language model (LLM). 'tidyprompt' primariy focused automatic interactions LLMs send_prompt() using tidyprompt object prompt_wrap(), class may useful manual conversation LLM. (may specifically used continue chat history returned send_prompt() return_mode = \"full\".)","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"PersistentChat R6 class — persistent_chat-class","text":"chat_history chat_history() object llm_provider llm_provider object","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"PersistentChat R6 class — persistent_chat-class","text":"persistent_chat-class$new() persistent_chat-class$chat() persistent_chat-class$reset_chat_history() persistent_chat-class$clone()","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"PersistentChat R6 class — persistent_chat-class","text":"Initialize PersistentChat object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PersistentChat R6 class — persistent_chat-class","text":"","code":"persistent_chat-class$new(llm_provider, chat_history = NULL)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"PersistentChat R6 class — persistent_chat-class","text":"llm_provider llm_provider object chat_history (optional) chat_history() object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"PersistentChat R6 class — persistent_chat-class","text":"initialized PersistentChat object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"PersistentChat R6 class — persistent_chat-class","text":"Add message chat history get response LLM","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"PersistentChat R6 class — persistent_chat-class","text":"","code":"persistent_chat-class$chat(msg, role = \"user\", verbose = TRUE)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"PersistentChat R6 class — persistent_chat-class","text":"msg Message add chat history role Role message verbose Whether print interaction console","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"PersistentChat R6 class — persistent_chat-class","text":"response LLM","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"method-reset-chat-history-","dir":"Reference","previous_headings":"","what":"Method reset_chat_history()","title":"PersistentChat R6 class — persistent_chat-class","text":"Reset chat history","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"PersistentChat R6 class — persistent_chat-class","text":"","code":"persistent_chat-class$reset_chat_history()"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"PersistentChat R6 class — persistent_chat-class","text":"NULL","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"PersistentChat R6 class — persistent_chat-class","text":"objects class cloneable method.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"PersistentChat R6 class — persistent_chat-class","text":"","code":"persistent_chat-class$clone(deep = FALSE)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"PersistentChat R6 class — persistent_chat-class","text":"deep Whether make deep clone.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/persistent_chat-class.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"PersistentChat R6 class — persistent_chat-class","text":"","code":"# Create a persistent chat with any LLM provider chat <- `persistent_chat-class`$new(llm_provider_ollama())  if (FALSE) { # \\dontrun{   chat$chat(\"Hi! Tell me about Twente, in a short sentence?\")   # --- Sending request to LLM provider (llama3.1:8b): ---   # Hi! Tell me about Twente, in a short sentence?   # --- Receiving response from LLM provider: ---   # Twente is a charming region in the Netherlands known for its picturesque   # countryside and vibrant culture!    chat$chat(\"How many people live there?\")   # --- Sending request to LLM provider (llama3.1:8b): ---   # How many people live there?   # --- Receiving response from LLM provider: ---   # The population of Twente is approximately 650,000 inhabitants, making it one of   # the largest regions in the Netherlands.    # Access the chat history:   chat$chat_history    # Reset the chat history:   chat$reset_chat_history()    # Continue a chat from the result of `send_prompt()`:   result <- \"Hi there!\" |>     answer_as_integer() |>     send_prompt(return_mode = \"full\")   # --- Sending request to LLM provider (llama3.1:8b): ---   # Hi there!   #   # You must answer with only an integer (use no other characters).   # --- Receiving response from LLM provider: ---   # 42   chat <- `persistent_chat-class`$new(llm_provider_ollama(), result$chat_history)   chat$chat(\"Why did you choose that number?\")   # --- Sending request to LLM provider (llama3.1:8b): ---   # Why did you choose that number?   # --- Receiving response from LLM provider: ---   # I chose the number 42 because it's a reference to Douglas Adams' science fiction   # series \"The Hitchhiker's Guide to the Galaxy,\" in which a supercomputer named   # Deep Thought is said to have calculated the \"Answer to the Ultimate Question of   # Life, the Universe, and Everything\" as 42. } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/prompt_wrap.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrap a prompt with functions for modification and handling the LLM response — prompt_wrap","title":"Wrap a prompt with functions for modification and handling the LLM response — prompt_wrap","text":"function takes single string tidyprompt object adds new prompt wrap . prompt wrap set functions modify prompt text, extract value LLM response, validate extracted value. functions used ensure prompt LLM response correct format meet specified criteria; may also used provide LLM feedback additional information, like result tool call evaluated code. Advanced prompt wraps may also include functions directly handle response LLM API configure API parameters.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/prompt_wrap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrap a prompt with functions for modification and handling the LLM response — prompt_wrap","text":"","code":"prompt_wrap(   prompt,   modify_fn = NULL,   extraction_fn = NULL,   validation_fn = NULL,   handler_fn = NULL,   parameter_fn = NULL,   type = c(\"unspecified\", \"mode\", \"tool\", \"break\", \"check\"),   name = NULL )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/prompt_wrap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrap a prompt with functions for modification and handling the LLM response — prompt_wrap","text":"prompt string tidyprompt object modify_fn function takes previous prompt text (first argument) returns new prompt text extraction_fn function takes LLM response (first argument) attempts extract value . Upon succesful extraction, function return extracted value. extraction fails, function return llm_feedback() message initiate retry. llm_break() can returned break extraction validation loop, ending send_prompt() validation_fn function takes (extracted) LLM response (first argument) attempts validate . Upon succesful validation, function return TRUE. validation fails, function return llm_feedback() message initiate retry. llm_break() can returned break extraction validation loop, ending send_prompt() handler_fn function takes 'completion' object (result request LLM, returned $complete_chat() llm_provider object) first argument llm_provider object second argument. function return (modified identical) completion object. can used advanced side effects, like logging, native tool calling, keeping track token usage. See llm_provider information; handler_fn attached llm_provider object used. example usage, see source code answer_using_tools() parameter_fn function takes llm_provider object used send_prompt() returns named list parameters set llm_provider object via $set_parameters() method. can used configure specific parameters llm_provider object evaluating prompt. example, answer_as_json() may set different parameters different APIs related JSON output. function typically used advanced prompt wraps require specific settings llm_provider object type type prompt wrap. Must one : \"unspecified\": default type, typically used prompt wraps request specific format LLM response, like answer_as_integer() \"mode\": prompt wraps change LLM answer prompt, like answer_by_chain_of_thought() answer_by_react() \"tool\": prompt wraps enable LLM use tools, like answer_using_tools() answer_using_r() 'output_as_tool' = TRUE \"break\": prompt wraps may break extraction validation loop, like quit_if(). applied type \"unspecified\" may instruct LLM answer prompt manner specified prompt wraps \"check\": prompt wraps apply last check final answer, prompt wraps evaluated. prompt wraps may contain validation function, applied prompt wraps evaluated. prompt wraps even applied earlier prompt wrap broken extraction validation loop llm_break() Types used determine order prompt wraps applied. constructing prompt text, prompt wraps applied base prompt following order: 'check', 'unspecified', 'break', 'mode', 'tool'. evaluating LLM response applying extraction validation functions, prompt wraps applied reverse order: 'tool', 'mode', 'break', 'unspecified', 'check'. Order among type preserved order added prompt. name optional name prompt wrap. can used identify prompt wrap tidyprompt object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/prompt_wrap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrap a prompt with functions for modification and handling the LLM response — prompt_wrap","text":"tidyprompt object prompt_wrap() appended ","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/prompt_wrap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Wrap a prompt with functions for modification and handling the LLM response — prompt_wrap","text":"advanced use, modify_fn, extraction_fn, validation_fn may take llm_provider object (used send_prompt()) second argument, 'http_list' (list HTTP requests responses made send_prompt()) third argument. Use arguments required, can useful complex prompt wraps require additional information LLM provider requests made far. functions (including parameter_fn) also access object self (function argument; attached environment function) contains tidyprompt object prompt wrap part . can used access prompt wraps, access prompt text information prompt. instance, prompt wraps can accessed self$get_prompt_wraps().","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/prompt_wrap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrap a prompt with functions for modification and handling the LLM response — prompt_wrap","text":"","code":"# A custom prompt_wrap may be created during piping prompt <- \"Hi there!\" |>   prompt_wrap(     modify_fn = function(base_prompt) {       paste(base_prompt, \"How are you?\", sep = \"\\n\\n\")     }   ) prompt #> <tidyprompt> #> The base prompt is modified by a prompt wrap, resulting in: #> > Hi there! #> >  #> > How are you?  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #> Use 'get_prompt_wraps(x)' to show the prompt wraps. #>   # (Shorter notation of the above:) prompt <- \"Hi there!\" |>   prompt_wrap(\\(x) paste(x, \"How are you?\", sep = \"\\n\\n\"))  # It may often be preferred to make a function which takes a prompt and #   returns a wrapped prompt: my_prompt_wrap <- function(prompt) {   modify_fn <- function(base_prompt) {     paste(base_prompt, \"How are you?\", sep = \"\\n\\n\")   }    prompt_wrap(prompt, modify_fn) } prompt <- \"Hi there!\" |>   my_prompt_wrap()  # For more advanced examples, take a look at the source code of the #   pre-built prompt wraps in the tidyprompt package, like #   answer_as_boolean, answer_as_integer, add_tools, answer_as_code, etc. # Below is the source code for the 'answer_as_integer' prompt wrap function:  #' Make LLM answer as an integer (between min and max) #' #' @param prompt A single string or a [tidyprompt()] object #' @param min (optional) Minimum value for the integer #' @param max (optional) Maximum value for the integer #' @param add_instruction_to_prompt (optional) Add instruction for replying #' as an integer to the prompt text. Set to FALSE for debugging if extractions/validations #' are working as expected (without instruction the answer should fail the #' validation function, initiating a retry) #' #' @return A [tidyprompt()] with an added [prompt_wrap()] which #' will ensure that the LLM response is an integer. #' #' @export #' #' @example inst/examples/answer_as_integer.R #' #' @family pre_built_prompt_wraps #' @family answer_as_prompt_wraps answer_as_integer <- function(     prompt,     min = NULL,     max = NULL,     add_instruction_to_prompt = TRUE ) {   instruction <- \"You must answer with only an integer (use no other characters).\"    if (!is.null(min) && !is.null(max)) {     instruction <- paste(instruction, glue::glue(       \"Enter an integer between {min} and {max}.\"     ))   } else if (!is.null(min)) {     instruction <- paste(instruction, glue::glue(       \"Enter an integer greater than or equal to {min}.\"     ))   } else if (!is.null(max)) {     instruction <- paste(instruction, glue::glue(       \"Enter an integer less than or equal to {max}.\"     ))   }    modify_fn <- function(original_prompt_text) {     if (!add_instruction_to_prompt) {       return(original_prompt_text)     }      glue::glue(\"{original_prompt_text}\\n\\n{instruction}\")   }    extraction_fn <- function(x) {     extracted <- suppressWarnings(as.numeric(x))     if (is.na(extracted)) {       return(llm_feedback(instruction))     }     return(extracted)   }    validation_fn <- function(x) {     if (x != floor(x)) { # Not a whole number       return(llm_feedback(instruction))     }      if (!is.null(min) && x < min) {       return(llm_feedback(glue::glue(         \"The number should be greater than or equal to {min}.\"       )))     }     if (!is.null(max) && x > max) {       return(llm_feedback(glue::glue(         \"The number should be less than or equal to {max}.\"       )))     }     return(TRUE)   }    prompt_wrap(     prompt,     modify_fn, extraction_fn, validation_fn,     name = \"answer_as_integer\"   ) }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/quit_if.html","id":null,"dir":"Reference","previous_headings":"","what":"Make evaluation of a prompt stop if LLM gives a specific response — quit_if","title":"Make evaluation of a prompt stop if LLM gives a specific response — quit_if","text":"function used wrap tidyprompt() object ensure evaluation stop LLM says answer prompt. useful scenarios determined LLM unable provide response prompt.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/quit_if.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make evaluation of a prompt stop if LLM gives a specific response — quit_if","text":"","code":"quit_if(   prompt,   quit_detect_regex = \"NO ANSWER\",   instruction =     paste0(\"If you think that you cannot provide a valid answer, you must type:\\n\",     \"'NO ANSWER' (use no other characters)\"),   success = TRUE,   response_result = c(\"null\", \"llm_response\", \"regex_match\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/quit_if.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make evaluation of a prompt stop if LLM gives a specific response — quit_if","text":"prompt single string tidyprompt() object quit_detect_regex regular expression detect LLM's response cause evaluation stop. default detect string \"ANSWER\" response instruction string added prompt instruct LLM respond answer prompt. default \"think provide valid answer, must type: 'ANSWER' (use characters)\". parameter can set NULL instruction needed prompt success logical indicating whether send_prompt() loop break nonetheless considered successful completion extraction validation process. FALSE, object_to_return must always set NULL thus parameter 'response_result' must also set 'null'; FALSE, send_prompt() also print warning unsuccessful evaluation. TRUE, object_to_return returned response result send_prompt() (send_prompt() print warning unsuccessful evaluation); parameter 'response_result' determine returned response result send_prompt(). response_result character string indicating returned quit_detect_regex detected LLM's response. default 'null', return NULL response result o f send_prompt(). 'llm_response', full LLM response returned response result send_prompt(). 'regex_match', part LLM response matches quit_detect_regex returned response result send_prompt()","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/quit_if.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make evaluation of a prompt stop if LLM gives a specific response — quit_if","text":"tidyprompt() added prompt_wrap() ensure evaluation stop upon detection quit_detect_regex LLM's response","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/quit_if.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make evaluation of a prompt stop if LLM gives a specific response — quit_if","text":"","code":"if (FALSE) { # \\dontrun{   \"What the favourite food of my cat on Thursday mornings?\" |>     quit_if() |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   #   What the favourite food of my cat on Thursday mornings?   #   #   If you think that you cannot provide a valid answer, you must type:   #   'NO ANSWER' (use no other characters)   # --- Receiving response from LLM provider: ---   #   NO ANSWER   # NULL } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/r_json_schema_to_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate an example object from a JSON schema — r_json_schema_to_example","title":"Generate an example object from a JSON schema — r_json_schema_to_example","text":"function generates example JSON object JSON schema. used enforcing JSON schema text-based handling (requiring example added prompt text).","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/r_json_schema_to_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate an example object from a JSON schema — r_json_schema_to_example","text":"","code":"r_json_schema_to_example(schema)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/r_json_schema_to_example.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate an example object from a JSON schema — r_json_schema_to_example","text":"schema list (R object) representing JSON schema","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/r_json_schema_to_example.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate an example object from a JSON schema — r_json_schema_to_example","text":"list (R object) matches JSON schema definition","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/r_json_schema_to_example.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate an example object from a JSON schema — r_json_schema_to_example","text":"","code":"base_prompt <- \"How can I solve 8x + 7 = -23?\"  # This example will show how to enforce JSON format in the response, #   with and without a schema, using the 'answer_as_json()' prompt wrap. # If you use type = 'auto', the function will automatically detect the #   best way to enforce JSON based on the LLM provider you are using. # Note that the default type is 'text-based', which will work for any provider/model  #### Enforcing JSON without a schema: ####  if (FALSE) { # \\dontrun{   ## Text-based (works for any provider/model):   #   Adds request to prompt for a JSON object   #   Extracts JSON from textual response (feedback for retry if no JSON received)   #   Parses JSON to R object   json_1 <- base_prompt |>     answer_as_json() |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   # How can I solve 8x + 7 = -23?   #   # Your must format your response as a JSON object.   # --- Receiving response from LLM provider: ---   # Here is the solution to the equation formatted as a JSON object:   #   # ```   # {   #   \"equation\": \"8x + 7 = -23\",   #   \"steps\": [   #     {   #       \"step\": \"Subtract 7 from both sides of the equation\",   #       \"expression\": \"-23 - 7\"   #     },   #     {   #       \"step\": \"Simplify the expression on the left side\",   #       \"result\": \"-30\"   #     },   #     {   #       \"step\": \"Divide both sides by -8 to solve for x\",   #       \"expression\": \"-30 / -8\"   #     },   #     {   #       \"step\": \"Simplify the expression on the right side\",   #       \"result\": \"3.75\"   #     }   #   ],   #   \"solution\": {   #     \"x\": 3.75   #   }   # }   # ```     ## Ollama:   #   - Sets 'format' parameter to 'json', enforcing JSON   #   - Adds request to prompt for a JSON object, as is recommended by the docs   #   - Parses JSON to R object   json_2 <- base_prompt |>     answer_as_json(type = \"auto\") |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   # How can I solve 8x + 7 = -23?   #   # Your must format your response as a JSON object.   # --- Receiving response from LLM provider: ---   # {\"steps\": [   #   \"Subtract 7 from both sides to get 8x = -30\",   #   \"Simplify the right side of the equation to get 8x = -30\",   #   \"Divide both sides by 8 to solve for x, resulting in x = -30/8\",   #   \"Simplify the fraction to find the value of x\"   # ],   # \"value_of_x\": \"-3.75\"}     ## OpenAI-type API without schema:   #   - Sets 'response_format' parameter to 'json_object', enforcing JSON   #   - Adds request to prompt for a JSON object, as is required by the API   #   - Parses JSON to R object   json_3 <- base_prompt |>     answer_as_json(type = \"auto\") |>     send_prompt(llm_provider_openai())   # --- Sending request to LLM provider (gpt-4o-mini): ---   # How can I solve 8x + 7 = -23?   #   # Your must format your response as a JSON object.   # --- Receiving response from LLM provider: ---   # {   #   \"solution_steps\": [   #     {   #       \"step\": 1,   #       \"operation\": \"Subtract 7 from both sides\",   #       \"equation\": \"8x + 7 - 7 = -23 - 7\",   #       \"result\": \"8x = -30\"   #     },   #     {   #       \"step\": 2,   #       \"operation\": \"Divide both sides by 8\",   #       \"equation\": \"8x / 8 = -30 / 8\",   #       \"result\": \"x = -3.75\"   #     }   #   ],   #   \"solution\": {   #     \"x\": -3.75   #   }   # } } # }    #### Enforcing JSON with a schema: ####  # Make a list representing a JSON schema, #   which the LLM response must adhere to: json_schema <- list(   name = \"steps_to_solve\", # Required for OpenAI API   description = NULL, # Optional for OpenAI API   schema = list(     type = \"object\",     properties = list(       steps = list(         type = \"array\",         items = list(           type = \"object\",           properties = list(             explanation = list(type = \"string\"),             output = list(type = \"string\")           ),           required = c(\"explanation\", \"output\"),           additionalProperties = FALSE         )       ),       final_answer = list(type = \"string\")     ),     required = c(\"steps\", \"final_answer\"),     additionalProperties = FALSE   )   # 'strict' parameter is set as argument 'answer_as_json()' ) # Note: when you are not using an OpenAI API, you can also pass just the #   internal 'schema' list object to 'answer_as_json()' instead of the full #   'json_schema' list object  # Generate example R object based on schema: r_json_schema_to_example(json_schema) #> $steps #> $steps[[1]] #> $steps[[1]]$explanation #> [1] \"...\" #>  #> $steps[[1]]$output #> [1] \"...\" #>  #>  #>  #> $final_answer #> [1] \"...\" #>   if (FALSE) { # \\dontrun{   ## Text-based with schema (works for any provider/model):   #   - Adds request to prompt for a JSON object   #   - Adds schema to prompt   #   - Extracts JSON from textual response (feedback for retry if no JSON received)   #   - Validates JSON against schema with 'jsonvalidate' package (feedback for retry if invalid)   #   - Parses JSON to R object   json_4 <- base_prompt |>     answer_as_json(schema = json_schema) |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   # How can I solve 8x + 7 = -23?   #   # Your must format your response as a JSON object.   #   # Your JSON object should match this example JSON object:   #   {   #     \"steps\": [   #       {   #         \"explanation\": \"...\",   #         \"output\": \"...\"   #       }   #     ],   #     \"final_answer\": \"...\"   #   }   # --- Receiving response from LLM provider: ---   # Here is the solution to the equation:   #   # ```   # {   #   \"steps\": [   #     {   #       \"explanation\": \"First, we want to isolate the term with 'x' by   #       subtracting 7 from both sides of the equation.\",   #       \"output\": \"8x + 7 - 7 = -23 - 7\"   #     },   #     {   #       \"explanation\": \"This simplifies to: 8x = -30\",   #       \"output\": \"8x = -30\"   #     },   #     {   #       \"explanation\": \"Next, we want to get rid of the coefficient '8' by   #       dividing both sides of the equation by 8.\",   #       \"output\": \"(8x) / 8 = (-30) / 8\"   #     },   #     {   #       \"explanation\": \"This simplifies to: x = -3.75\",   #       \"output\": \"x = -3.75\"   #     }   #   ],   #   \"final_answer\": \"-3.75\"   # }   # ```     ## Ollama with schema:   #   - Sets 'format' parameter to 'json', enforcing JSON   #   - Adds request to prompt for a JSON object, as is recommended by the docs   #   - Adds schema to prompt   #   - Validates JSON against schema with 'jsonvalidate' package (feedback for retry if invalid)   json_5 <- base_prompt |>     answer_as_json(json_schema, type = \"auto\") |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   # How can I solve 8x + 7 = -23?   #   # Your must format your response as a JSON object.   #   # Your JSON object should match this example JSON object:   # {   #   \"steps\": [   #     {   #       \"explanation\": \"...\",   #       \"output\": \"...\"   #     }   #   ],   #   \"final_answer\": \"...\"   # }   # --- Receiving response from LLM provider: ---   # {   #   \"steps\": [   #     {   #       \"explanation\": \"First, subtract 7 from both sides of the equation to   #       isolate the term with x.\",   #       \"output\": \"8x = -23 - 7\"   #     },   #     {   #       \"explanation\": \"Simplify the right-hand side of the equation.\",   #       \"output\": \"8x = -30\"   #     },   #     {   #       \"explanation\": \"Next, divide both sides of the equation by 8 to solve for x.\",   #       \"output\": \"x = -30 / 8\"   #     },   #     {   #       \"explanation\": \"Simplify the right-hand side of the equation.\",   #       \"output\": \"x = -3.75\"   #     }   #   ],   #   \"final_answer\": \"-3.75\"   # }    ## OpenAI with schema:   #   - Sets 'response_format' parameter to 'json_object', enforcing JSON   #   - Adds json_schema to the API request, API enforces JSON adhering schema   #   - Parses JSON to R object   json_6 <- base_prompt |>     answer_as_json(json_schema, type = \"auto\") |>     send_prompt(llm_provider_openai())   # --- Sending request to LLM provider (gpt-4o-mini): ---   # How can I solve 8x + 7 = -23?   # --- Receiving response from LLM provider: ---   # {\"steps\":[   # {\"explanation\":\"Start with the original equation.\",   # \"output\":\"8x + 7 = -23\"},   # {\"explanation\":\"Subtract 7 from both sides to isolate the term with x.\",   # \"output\":\"8x + 7 - 7 = -23 - 7\"},   # {\"explanation\":\"Simplify the left side and the right side of the equation.\",   # \"output\":\"8x = -30\"},   # {\"explanation\":\"Now, divide both sides by 8 to solve for x.\",   # \"output\":\"x = -30 / 8\"},   # {\"explanation\":\"Simplify the fraction by dividing both the numerator and the   # denominator by 2.\",   # \"output\":\"x = -15 / 4\"}   # ], \"final_answer\":\"x = -15/4\"} } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/send_prompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Send a prompt to a LLM provider — send_prompt","title":"Send a prompt to a LLM provider — send_prompt","text":"function responsible sending prompts LLM provider evaluation. function interact LLM provider successful response received maximum number interactions reached. function apply extraction validation functions LLM response, specified prompt wraps (see prompt_wrap()). maximum number interactions","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/send_prompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send a prompt to a LLM provider — send_prompt","text":"","code":"send_prompt(   prompt,   llm_provider = llm_provider_ollama(),   max_interactions = 10,   clean_chat_history = TRUE,   verbose = NULL,   stream = NULL,   return_mode = c(\"only_response\", \"full\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/send_prompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send a prompt to a LLM provider — send_prompt","text":"prompt string tidyprompt object llm_provider llm_provider object (default llm_provider_ollama()). object settings used evaluate prompt. Note 'verbose' 'stream' settings LLM provider overruled 'verbose' 'stream' arguments function NULL. Furthermore, advanced tidyprompt objects may carry '$parameter_fn' functions can set parameters llm_provider object (see prompt_wrap() llm_provider ). max_interactions Maximum number interactions allowed LLM provider. Default 10. maximum number interactions reached without successful response, 'NULL' returned response (see return value). first interaction initial chat completion clean_chat_history chat history cleaned interaction. Cleaning chat history means first last message user, last message assistant, messages system, tool results kept 'clean' chat history. clean chat history used requesting new chat completion. (.e., LLM repeatedly fails provide correct response, last failed response included context window). may increase LLM performance next interaction verbose interaction LLM provider printed console. overrule 'verbose' setting LLM provider stream interaction LLM provider streamed. setting used LLM provider already 'stream' parameter (indicates support streaming). Note 'verbose' set FALSE, 'stream' setting ignored return_mode One 'full' 'only_response'. See return value","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/send_prompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send a prompt to a LLM provider — send_prompt","text":"return mode 'only_response', function return LLM response extraction validation functions applied (NULL returned unsuccessful maximum number interactions). return mode 'full', function return list following elements: 'response' (LLM response extraction validation functions applied; NULL returned unsuccessful maximum number interactions), 'interactions' (number interactions LLM provider), 'chat_history' (dataframe full chat history led final response), 'chat_history_clean' (dataframe cleaned chat history led final response; , first last message user, last message assistant, messages system kept), 'start_time' (time function called), 'end_time' (time function ended), 'duration_seconds' (duration function seconds), 'http_list' (list HTTP responses made interactions; returned llm_provider$complete_chat()).","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/send_prompt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Send a prompt to a LLM provider — send_prompt","text":"","code":"if (FALSE) { # \\dontrun{   \"Hi!\" |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   #   Hi!   # --- Receiving response from LLM provider: ---   #   It's nice to meet you. Is there something I can help you with, or would you like to chat?   # [1] \"It's nice to meet you. Is there something I can help you with, or would you like to chat?\"    \"Hi!\" |>     send_prompt(llm_provider_ollama(), return_mode = \"full\")   # --- Sending request to LLM provider (llama3.1:8b): ---   #   Hi!   # --- Receiving response from LLM provider: ---   #   It's nice to meet you. Is there something I can help you with, or would you like to chat?   # $response   # [1] \"It's nice to meet you. Is there something I can help you with, or would you like to chat?\"   #   # $chat_history   # ...   #   # $chat_history_clean   # ...   #   # $start_time   # [1] \"2024-11-18 15:43:12 CET\"   #   # $end_time   # [1] \"2024-11-18 15:43:13 CET\"   #   # $duration_seconds   # [1] 1.13276   #   # $http_list   # $http_list[[1]]   # Response [http://localhost:11434/api/chat]   #   Date: 2024-11-18 14:43   #   Status: 200   #   Content-Type: application/x-ndjson   # <EMPTY BODY>    \"Hi!\" |>     add_text(\"What is 5 + 5?\") |>     answer_as_integer() |>     send_prompt(llm_provider_ollama(), verbose = FALSE)   # [1] 10 } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/set_chat_history.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the chat history of a tidyprompt object — set_chat_history","title":"Set the chat history of a tidyprompt object — set_chat_history","text":"function sets chat history tidyprompt object. chat history also set base prompt system prompt (last message chat history role 'user' used base prompt; first message chat history may role 'system' used system prompt). may useful one wants change base prompt, system prompt, chat history tidyprompt object retaining fields like list prompt wraps.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/set_chat_history.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the chat history of a tidyprompt object — set_chat_history","text":"","code":"set_chat_history(x, chat_history)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/set_chat_history.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the chat history of a tidyprompt object — set_chat_history","text":"x tidyprompt object chat_history valid chat history (see chat_history())","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/set_chat_history.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the chat history of a tidyprompt object — set_chat_history","text":"updated tidyprompt object","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/set_chat_history.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the chat history of a tidyprompt object — set_chat_history","text":"","code":"prompt <- tidyprompt(\"Hi!\") print(prompt) #> <tidyprompt> #> The base prompt is not modified by prompt wraps: #> > Hi!  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #>   # Add to a tidyprompt using a prompt wrap: prompt <- tidyprompt(\"Hi!\") |>   add_text(\"How are you?\") print(prompt) #> <tidyprompt> #> The base prompt is modified by a prompt wrap, resulting in: #> > Hi! #> >  #> > How are you?  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #> Use 'get_prompt_wraps(x)' to show the prompt wraps. #>   # Strings can be input for prompt wraps; therefore, #   a call to tidyprompt() is not necessary: prompt <- \"Hi\" |>   add_text(\"How are you?\")  # Example of adding extraction & validation with a prompt_wrap(): prompt <- \"Hi\" |>   add_text(\"What is 5 + 5?\") |>   answer_as_integer()  if (FALSE) { # \\dontrun{   # tidyprompt objects are evaluated by send_prompt(), which will   #   handle construct the prompt text, send it to the LLM provider,   #   and apply the extraction and validation functions from the tidyprompt object   prompt |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   #   Hi   #   #   What is 5 + 5?   #   #   You must answer with only an integer (use no other characters).   # --- Receiving response from LLM provider: ---   #   10   # [1] 10    # See prompt_wrap() and send_prompt() for more details } # }  # `tidyprompt` objects may be validated with these helpers: is_tidyprompt(prompt) # Returns TRUE if input is a valid tidyprompt object #> [1] TRUE  # Get base prompt text base_prompt <- prompt$base_prompt  # Get all prompt wraps prompt_wraps <- prompt$get_prompt_wraps() # Alternative: prompt_wraps <- get_prompt_wraps(prompt)  # Construct prompt text prompt_text <- prompt$construct_prompt_text() # Alternative: prompt_text <- construct_prompt_text(prompt)  # Set chat history (affecting also the base prompt) chat_history <- data.frame(   role = c(\"user\", \"assistant\", \"user\"),   content = c(\"What is 5 + 5?\", \"10\", \"And what is 5 + 6?\") ) prompt$set_chat_history(chat_history)  # Get chat history chat_history <- prompt$get_chat_history()"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/set_system_prompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Set system prompt of a tidyprompt object — set_system_prompt","title":"Set system prompt of a tidyprompt object — set_system_prompt","text":"Set system prompt prompt. system prompt added message role 'system' start chat history prompt evaluated send_prompt().","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/set_system_prompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set system prompt of a tidyprompt object — set_system_prompt","text":"","code":"set_system_prompt(prompt, system_prompt)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/set_system_prompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set system prompt of a tidyprompt object — set_system_prompt","text":"prompt single string tidyprompt() object system_prompt single character string representing system prompt","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/set_system_prompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set system prompt of a tidyprompt object — set_system_prompt","text":"tidyprompt() system prompt set","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/set_system_prompt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set system prompt of a tidyprompt object — set_system_prompt","text":"system prompt stored tidyprompt() object '$system_prompt'.","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/set_system_prompt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set system prompt of a tidyprompt object — set_system_prompt","text":"","code":"prompt <- \"Hi there!\" |>   set_system_prompt(\"You are an assistant who always answers in very short poems.\") prompt$system_prompt #> [1] \"You are an assistant who always answers in very short poems.\"  if (FALSE) { # \\dontrun{   prompt |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   #   Hi there!   # --- Receiving response from LLM provider: ---   #   Hello to you, I say,   #   Welcome here, come what may!   #   How can I assist today?   # [1] \"Hello to you, I say,\\nWelcome here, come what may!\\nHow can I assist today?\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/skim_with_labels_and_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Skim a dataframe and include labels and levels — skim_with_labels_and_levels","title":"Skim a dataframe and include labels and levels — skim_with_labels_and_levels","text":"function takes data.frame returns skim summary variable names, labels, levels categorical variables. wrapper around skimr::skim() function.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/skim_with_labels_and_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Skim a dataframe and include labels and levels — skim_with_labels_and_levels","text":"","code":"skim_with_labels_and_levels(data)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/skim_with_labels_and_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Skim a dataframe and include labels and levels — skim_with_labels_and_levels","text":"data data.frame skimmed","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/skim_with_labels_and_levels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Skim a dataframe and include labels and levels — skim_with_labels_and_levels","text":"data.frame variable names, labels, levels, skim summary","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/skim_with_labels_and_levels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Skim a dataframe and include labels and levels — skim_with_labels_and_levels","text":"","code":"# First add some labels to 'mtcars': mtcars$car <- rownames(mtcars) mtcars$car <- factor(mtcars$car, levels = rownames(mtcars)) attr(mtcars$car, \"label\") <- \"Name of the car\"  # Then skim the data: mtcars |>   skim_with_labels_and_levels() #>    variable     description       levels skim_type n_missing complete_rate #> 1        am            <NA>           NA   numeric         0             1 #> 2       car Name of the car Mazda RX....    factor         0             1 #> 3      carb            <NA>           NA   numeric         0             1 #> 4       cyl            <NA>           NA   numeric         0             1 #> 5      disp            <NA>           NA   numeric         0             1 #> 6      drat            <NA>           NA   numeric         0             1 #> 7      gear            <NA>           NA   numeric         0             1 #> 8        hp            <NA>           NA   numeric         0             1 #> 9       mpg            <NA>           NA   numeric         0             1 #> 10     qsec            <NA>           NA   numeric         0             1 #> 11       vs            <NA>           NA   numeric         0             1 #> 12       wt            <NA>           NA   numeric         0             1 #>    factor.ordered factor.n_unique              factor.top_counts numeric.mean #> 1              NA              NA                           <NA>     0.406250 #> 2           FALSE              32 Maz: 1, Maz: 1, Dat: 1, Hor: 1           NA #> 3              NA              NA                           <NA>     2.812500 #> 4              NA              NA                           <NA>     6.187500 #> 5              NA              NA                           <NA>   230.721875 #> 6              NA              NA                           <NA>     3.596563 #> 7              NA              NA                           <NA>     3.687500 #> 8              NA              NA                           <NA>   146.687500 #> 9              NA              NA                           <NA>    20.090625 #> 10             NA              NA                           <NA>    17.848750 #> 11             NA              NA                           <NA>     0.437500 #> 12             NA              NA                           <NA>     3.217250 #>     numeric.sd numeric.p0 numeric.p25 numeric.p50 numeric.p75 numeric.p100 #> 1    0.4989909      0.000     0.00000       0.000        1.00        1.000 #> 2           NA         NA          NA          NA          NA           NA #> 3    1.6152000      1.000     2.00000       2.000        4.00        8.000 #> 4    1.7859216      4.000     4.00000       6.000        8.00        8.000 #> 5  123.9386938     71.100   120.82500     196.300      326.00      472.000 #> 6    0.5346787      2.760     3.08000       3.695        3.92        4.930 #> 7    0.7378041      3.000     3.00000       4.000        4.00        5.000 #> 8   68.5628685     52.000    96.50000     123.000      180.00      335.000 #> 9    6.0269481     10.400    15.42500      19.200       22.80       33.900 #> 10   1.7869432     14.500    16.89250      17.710       18.90       22.900 #> 11   0.5040161      0.000     0.00000       0.000        1.00        1.000 #> 12   0.9784574      1.513     2.58125       3.325        3.61        5.424 #>    numeric.hist #> 1         ▇▁▁▁▆ #> 2          <NA> #> 3         ▇▂▅▁▁ #> 4         ▆▁▃▁▇ #> 5         ▇▃▃▃▂ #> 6         ▇▃▇▅▁ #> 7         ▇▁▆▁▂ #> 8         ▇▇▆▃▁ #> 9         ▃▇▅▁▂ #> 10        ▃▇▇▂▁ #> 11        ▇▁▁▁▆ #> 12        ▃▃▇▁▂"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidyprompt R6 Class — tidyprompt-class","title":"Tidyprompt R6 Class — tidyprompt-class","text":"tidyprompt object contains base prompt list prompt_wrap() objects. provides structured methods modify prompt simultaneously adding logic extract validate LLM response. Besides base prompt, tidyprompt object may contain system prompt chat history precede base prompt.","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Tidyprompt R6 Class — tidyprompt-class","text":"base_prompt base prompt string. base prompt modified prompt wraps construct_prompt_text(); modified prompt text used final message role 'user' send_prompt() system_prompt system prompt string. added start chat history role 'system' send_prompt()","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Tidyprompt R6 Class — tidyprompt-class","text":"tidyprompt-class$new() tidyprompt-class$is_valid() tidyprompt-class$add_prompt_wrap() tidyprompt-class$get_prompt_wraps() tidyprompt-class$construct_prompt_text() tidyprompt-class$set_chat_history() tidyprompt-class$get_chat_history() tidyprompt-class$clone()","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Tidyprompt R6 Class — tidyprompt-class","text":"Initialize tidyprompt object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tidyprompt R6 Class — tidyprompt-class","text":"","code":"tidyprompt-class$new(input)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tidyprompt R6 Class — tidyprompt-class","text":"input string, chat history, list containing chat history key '$chat_history', tidyprompt object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tidyprompt R6 Class — tidyprompt-class","text":"Different types input accepted initialization tidyprompt object: single character string. used base prompt dataframe valid chat history (see chat_history()) list containing valid chat history '$chat_history' (e.g., result send_prompt() using 'return_mode' = \"full\") tidyprompt object. checked validity , valid, fields copied object returned method passing dataframe list chat history, last row chat history must role 'user'; row used base prompt. first row chat history role 'system', used system prompt.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Tidyprompt R6 Class — tidyprompt-class","text":"tidyprompt object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"method-is-valid-","dir":"Reference","previous_headings":"","what":"Method is_valid()","title":"Tidyprompt R6 Class — tidyprompt-class","text":"Check tidyprompt object valid.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Tidyprompt R6 Class — tidyprompt-class","text":"","code":"tidyprompt-class$is_valid()"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Tidyprompt R6 Class — tidyprompt-class","text":"TRUE valid, otherwise FALSE","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"method-add-prompt-wrap-","dir":"Reference","previous_headings":"","what":"Method add_prompt_wrap()","title":"Tidyprompt R6 Class — tidyprompt-class","text":"Add prompt_wrap() tidyprompt object.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Tidyprompt R6 Class — tidyprompt-class","text":"","code":"tidyprompt-class$add_prompt_wrap(prompt_wrap)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tidyprompt R6 Class — tidyprompt-class","text":"prompt_wrap prompt_wrap() object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Tidyprompt R6 Class — tidyprompt-class","text":"updated tidyprompt object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"method-get-prompt-wraps-","dir":"Reference","previous_headings":"","what":"Method get_prompt_wraps()","title":"Tidyprompt R6 Class — tidyprompt-class","text":"Get list prompt_wrap() objects tidyprompt object.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Tidyprompt R6 Class — tidyprompt-class","text":"","code":"tidyprompt-class$get_prompt_wraps(   order = c(\"default\", \"modification\", \"evaluation\") )"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tidyprompt R6 Class — tidyprompt-class","text":"order order return wraps. Options : \"default\": originally added object \"modification\": ordered modification base prompt; ordered type: check, unspecified, mode, tool, break. order prompt wraps applied construct_prompt_text() \"evaluation\": ordered evaluation LLM response; ordered type: tool, mode, break, unspecified, check. order wraps applied LLM output send_prompt()","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Tidyprompt R6 Class — tidyprompt-class","text":"list prompt_wrap() objects.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"method-construct-prompt-text-","dir":"Reference","previous_headings":"","what":"Method construct_prompt_text()","title":"Tidyprompt R6 Class — tidyprompt-class","text":"Construct complete prompt text.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Tidyprompt R6 Class — tidyprompt-class","text":"","code":"tidyprompt-class$construct_prompt_text(llm_provider = NULL)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tidyprompt R6 Class — tidyprompt-class","text":"llm_provider Optional llm_provider object. may sometimes affect prompt text construction","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Tidyprompt R6 Class — tidyprompt-class","text":"string representing constructed prompt text","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"method-set-chat-history-","dir":"Reference","previous_headings":"","what":"Method set_chat_history()","title":"Tidyprompt R6 Class — tidyprompt-class","text":"function sets chat history tidyprompt object. chat history also set base prompt system prompt (last message chat history role 'user' used base prompt; first message chat history may role 'system' used system prompt). may useful one wants change base prompt, system prompt, chat history tidyprompt object retaining fields like prompt wraps.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Tidyprompt R6 Class — tidyprompt-class","text":"","code":"tidyprompt-class$set_chat_history(chat_history)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tidyprompt R6 Class — tidyprompt-class","text":"chat_history valid chat history (see chat_history())","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Tidyprompt R6 Class — tidyprompt-class","text":"updated tidyprompt object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"method-get-chat-history-","dir":"Reference","previous_headings":"","what":"Method get_chat_history()","title":"Tidyprompt R6 Class — tidyprompt-class","text":"function gets chat history tidyprompt object. chat history constructed base prompt, system prompt, chat history field. returned object chat history system prompt first message role 'system' base prompt last message role 'user'.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Tidyprompt R6 Class — tidyprompt-class","text":"","code":"tidyprompt-class$get_chat_history()"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Tidyprompt R6 Class — tidyprompt-class","text":"dataframe containing chat history","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Tidyprompt R6 Class — tidyprompt-class","text":"objects class cloneable method.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Tidyprompt R6 Class — tidyprompt-class","text":"","code":"tidyprompt-class$clone(deep = FALSE)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tidyprompt R6 Class — tidyprompt-class","text":"deep Whether make deep clone.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-class.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tidyprompt R6 Class — tidyprompt-class","text":"","code":"prompt <- tidyprompt(\"Hi!\") print(prompt) #> <tidyprompt> #> The base prompt is not modified by prompt wraps: #> > Hi!  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #>   # Add to a tidyprompt using a prompt wrap: prompt <- tidyprompt(\"Hi!\") |>   add_text(\"How are you?\") print(prompt) #> <tidyprompt> #> The base prompt is modified by a prompt wrap, resulting in: #> > Hi! #> >  #> > How are you?  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #> Use 'get_prompt_wraps(x)' to show the prompt wraps. #>   # Strings can be input for prompt wraps; therefore, #   a call to tidyprompt() is not necessary: prompt <- \"Hi\" |>   add_text(\"How are you?\")  # Example of adding extraction & validation with a prompt_wrap(): prompt <- \"Hi\" |>   add_text(\"What is 5 + 5?\") |>   answer_as_integer()  if (FALSE) { # \\dontrun{   # tidyprompt objects are evaluated by send_prompt(), which will   #   handle construct the prompt text, send it to the LLM provider,   #   and apply the extraction and validation functions from the tidyprompt object   prompt |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   #   Hi   #   #   What is 5 + 5?   #   #   You must answer with only an integer (use no other characters).   # --- Receiving response from LLM provider: ---   #   10   # [1] 10    # See prompt_wrap() and send_prompt() for more details } # }  # `tidyprompt` objects may be validated with these helpers: is_tidyprompt(prompt) # Returns TRUE if input is a valid tidyprompt object #> [1] TRUE  # Get base prompt text base_prompt <- prompt$base_prompt  # Get all prompt wraps prompt_wraps <- prompt$get_prompt_wraps() # Alternative: prompt_wraps <- get_prompt_wraps(prompt)  # Construct prompt text prompt_text <- prompt$construct_prompt_text() # Alternative: prompt_text <- construct_prompt_text(prompt)  # Set chat history (affecting also the base prompt) chat_history <- data.frame(   role = c(\"user\", \"assistant\", \"user\"),   content = c(\"What is 5 + 5?\", \"10\", \"And what is 5 + 6?\") ) prompt$set_chat_history(chat_history)  # Get chat history chat_history <- prompt$get_chat_history()"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-package.html","id":null,"dir":"Reference","previous_headings":"","what":"tidyprompt: Prompt Large Language Models and Enhance Their Functionality — tidyprompt-package","title":"tidyprompt: Prompt Large Language Models and Enhance Their Functionality — tidyprompt-package","text":"Easily construct prompts associated logic interacting large language models ('LLMs'). 'tidyprompt' introduces concept prompt wraps, building blocks can use quickly turn simple prompt complex one. Prompt wraps just modify prompt text, also add extraction validation functions applied response LLM. ensures user gets desired output. 'tidyprompt' can add various features prompts evaluation LLMs, structured output, automatic feedback, retries, reasoning modes, autonomous R function calling, R code generation evaluation. designed compatible LLM provider offers chat completion.","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"tidyprompt: Prompt Large Language Models and Enhance Their Functionality — tidyprompt-package","text":"Maintainer: Luka Koning l.koning@kennispunttwente.nl [copyright holder] Authors: Tjark Van de Merwe t.vandemerwe@kennispunttwente.nl [copyright holder] contributors: Kennispunt Twente info@kennispunttwente.nl [funder]","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a tidyprompt object — tidyprompt","title":"Create a tidyprompt object — tidyprompt","text":"wrapper around tidyprompt constructor.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a tidyprompt object — tidyprompt","text":"","code":"tidyprompt(input)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a tidyprompt object — tidyprompt","text":"input string, chat history, list containing chat history key '$chat_history', tidyprompt object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a tidyprompt object — tidyprompt","text":"tidyprompt object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a tidyprompt object — tidyprompt","text":"Different types input accepted initialization tidyprompt object: single character string. used base prompt dataframe valid chat history (see chat_history()) list containing valid chat history '$chat_history' (e.g., result send_prompt() using 'return_mode' = \"full\") tidyprompt object. checked validity , valid, fields copied object returned method passing dataframe list chat history, last row chat history must role 'user'; row used base prompt. first row chat history role 'system', used system prompt.","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tidyprompt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a tidyprompt object — tidyprompt","text":"","code":"prompt <- tidyprompt(\"Hi!\") print(prompt) #> <tidyprompt> #> The base prompt is not modified by prompt wraps: #> > Hi!  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #>   # Add to a tidyprompt using a prompt wrap: prompt <- tidyprompt(\"Hi!\") |>   add_text(\"How are you?\") print(prompt) #> <tidyprompt> #> The base prompt is modified by a prompt wrap, resulting in: #> > Hi! #> >  #> > How are you?  #> Use 'x$base_prompt' to show the base prompt text. #> Use 'x$construct_prompt_text()' to get the full prompt text. #> Use 'get_prompt_wraps(x)' to show the prompt wraps. #>   # Strings can be input for prompt wraps; therefore, #   a call to tidyprompt() is not necessary: prompt <- \"Hi\" |>   add_text(\"How are you?\")  # Example of adding extraction & validation with a prompt_wrap(): prompt <- \"Hi\" |>   add_text(\"What is 5 + 5?\") |>   answer_as_integer()  if (FALSE) { # \\dontrun{   # tidyprompt objects are evaluated by send_prompt(), which will   #   handle construct the prompt text, send it to the LLM provider,   #   and apply the extraction and validation functions from the tidyprompt object   prompt |>     send_prompt(llm_provider_ollama())   # --- Sending request to LLM provider (llama3.1:8b): ---   #   Hi   #   #   What is 5 + 5?   #   #   You must answer with only an integer (use no other characters).   # --- Receiving response from LLM provider: ---   #   10   # [1] 10    # See prompt_wrap() and send_prompt() for more details } # }  # `tidyprompt` objects may be validated with these helpers: is_tidyprompt(prompt) # Returns TRUE if input is a valid tidyprompt object #> [1] TRUE  # Get base prompt text base_prompt <- prompt$base_prompt  # Get all prompt wraps prompt_wraps <- prompt$get_prompt_wraps() # Alternative: prompt_wraps <- get_prompt_wraps(prompt)  # Construct prompt text prompt_text <- prompt$construct_prompt_text() # Alternative: prompt_text <- construct_prompt_text(prompt)  # Set chat history (affecting also the base prompt) chat_history <- data.frame(   role = c(\"user\", \"assistant\", \"user\"),   content = c(\"What is 5 + 5?\", \"10\", \"And what is 5 + 6?\") ) prompt$set_chat_history(chat_history)  # Get chat history chat_history <- prompt$get_chat_history()"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tools_add_docs.html","id":null,"dir":"Reference","previous_headings":"","what":"Add tidyprompt function documentation to a function — tools_add_docs","title":"Add tidyprompt function documentation to a function — tools_add_docs","text":"function adds documentation custom function. documentation used extract information function's name, description, arguments, return value. information used provide LLM information functions, LLM can call R functions. intended use function add documentation custom functions help files; tools_get_docs() may generate documentation help file function part base R package. function already documentation, documentation added function may overwrite . wish modify existing documentation, may make call tools_get_docs() extract existing documentation, modify , call tools_add_docs() add modified documentation.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tools_add_docs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add tidyprompt function documentation to a function — tools_add_docs","text":"","code":"tools_add_docs(func, docs)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tools_add_docs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add tidyprompt function documentation to a function — tools_add_docs","text":"func function object docs list following elements: 'name': (optional) name function. provided, function name extracted function object. Use parameter override function name necessary 'description': description function purpose 'arguments': named list arguments descriptions. argument list may contain: 'description': description argument purpose. required used native function calling (e.g., OpenAI), recommended text-based function calling 'type': type argument. one : 'integer', 'numeric', 'logical', 'string', 'match.arg', 'vector integer', 'vector numeric', 'vector logical', 'vector string'. arguments named lists, 'type' named list contains types elements. type 'match.arg', possible values passed vector 'default_value'. 'type' required native function calling (, e.g., OpenAI) may also useful provide text-based function calling, added prompt introducing function 'default_value': default value argument. required 'type' set 'match.arg'. vector possible values argument. cases, required; native function calling, used cases; text-based function calling, may useful provide default value, added prompt introducing function 'return': list following elements: 'description': description return value side effects function","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tools_add_docs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add tidyprompt function documentation to a function — tools_add_docs","text":"function object documentation added attribute ('tidyprompt_tool_docs')","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tools_add_docs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add tidyprompt function documentation to a function — tools_add_docs","text":"","code":"if (FALSE) { # \\dontrun{   # When using functions from base R or R packages,   #   documentation is automatically extracted from help files:   \"What are the files in my current directory?\" |>     answer_using_tools(dir) |> # 'dir' function is from base R     send_prompt() } # }  # Custom functions may also be provided; #   in this case, some documentation is extracted from the function's formals; #   descriptions may be added manually. See below  # Example fake weather function to add to the prompt: temperature_in_location <- function(     location = c(\"Amsterdam\", \"Utrecht\", \"Enschede\"),     unit = c(\"Celcius\", \"Fahrenheit\") ) {   location <- match.arg(location)   unit <- match.arg(unit)    temperature_celcius <- switch(     location,     \"Amsterdam\" = 32.5,     \"Utrecht\" = 19.8,     \"Enschede\" = 22.7   )    if (unit == \"Celcius\") {     return(temperature_celcius)   } else {     return(temperature_celcius * 9/5 + 32)   } }  # Generate documentation for a function #   (based on formals, & help file if available) docs <- tools_get_docs(temperature_in_location)  # The types get inferred from the function's formals # However, descriptions are still missing as the function is not from a package # We can modify the documentation object to add descriptions: docs$description <- \"Get the temperature in a location\" docs$arguments$unit$description <- \"Unit in which to return the temperature\" docs$arguments$location$description <- \"Location for which to return the temperature\" docs$return$description <- \"The temperature in the specified location and unit\" # (See `?tools_add_docs` for more details on the structure of the documentation)  # When we are satisfied with the documentation, we can add it to the function: temperature_in_location <- tools_add_docs(temperature_in_location, docs)  if (FALSE) { # \\dontrun{   # Now the LLM can use the function:   \"Hi, what is the weather in Enschede? Give me Celcius degrees\" |>     answer_using_tools(temperature_in_location) |>     send_prompt() } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tools_get_docs.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract documentation from a function — tools_get_docs","title":"Extract documentation from a function — tools_get_docs","text":"function extracts documentation help file (available, .e., function part package) documentation added tools_add_docs(). extracted documentation includes function's name, description, arguments, return value. information used provide LLM information functions, LLM can call R functions.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tools_get_docs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract documentation from a function — tools_get_docs","text":"","code":"tools_get_docs(func, name = NULL)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tools_get_docs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract documentation from a function — tools_get_docs","text":"func function object. function belong package documentation available help file, documentation added tools_add_docs() name name function already known (optional). provided extracted documentation function object's name","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tools_get_docs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract documentation from a function — tools_get_docs","text":"list documentation function. See tools_add_docs() information contents","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tools_get_docs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract documentation from a function — tools_get_docs","text":"function prioritize documentation added tools_add_docs() documentation help file. Thus, possible override help file documentation adding custom documentation","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/tools_get_docs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract documentation from a function — tools_get_docs","text":"","code":"if (FALSE) { # \\dontrun{   # When using functions from base R or R packages,   #   documentation is automatically extracted from help files:   \"What are the files in my current directory?\" |>     answer_using_tools(dir) |> # 'dir' function is from base R     send_prompt() } # }  # Custom functions may also be provided; #   in this case, some documentation is extracted from the function's formals; #   descriptions may be added manually. See below  # Example fake weather function to add to the prompt: temperature_in_location <- function(     location = c(\"Amsterdam\", \"Utrecht\", \"Enschede\"),     unit = c(\"Celcius\", \"Fahrenheit\") ) {   location <- match.arg(location)   unit <- match.arg(unit)    temperature_celcius <- switch(     location,     \"Amsterdam\" = 32.5,     \"Utrecht\" = 19.8,     \"Enschede\" = 22.7   )    if (unit == \"Celcius\") {     return(temperature_celcius)   } else {     return(temperature_celcius * 9/5 + 32)   } }  # Generate documentation for a function #   (based on formals, & help file if available) docs <- tools_get_docs(temperature_in_location)  # The types get inferred from the function's formals # However, descriptions are still missing as the function is not from a package # We can modify the documentation object to add descriptions: docs$description <- \"Get the temperature in a location\" docs$arguments$unit$description <- \"Unit in which to return the temperature\" docs$arguments$location$description <- \"Location for which to return the temperature\" docs$return$description <- \"The temperature in the specified location and unit\" # (See `?tools_add_docs` for more details on the structure of the documentation)  # When we are satisfied with the documentation, we can add it to the function: temperature_in_location <- tools_add_docs(temperature_in_location, docs)  if (FALSE) { # \\dontrun{   # Now the LLM can use the function:   \"Hi, what is the weather in Enschede? Give me Celcius degrees\" |>     answer_using_tools(temperature_in_location) |>     send_prompt() } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/user_verify.html","id":null,"dir":"Reference","previous_headings":"","what":"Have user check the result of a prompt (human-in-the-loop) — user_verify","title":"Have user check the result of a prompt (human-in-the-loop) — user_verify","text":"function used user check result prompt. evaluation prompt applying prompt wraps, user presented result asked accept decline. user declines, asked provide feedback large language model (LLM) LLM can retry prompt.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/user_verify.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Have user check the result of a prompt (human-in-the-loop) — user_verify","text":"","code":"user_verify(prompt)"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/user_verify.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Have user check the result of a prompt (human-in-the-loop) — user_verify","text":"prompt single string tidyprompt object","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/user_verify.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Have user check the result of a prompt (human-in-the-loop) — user_verify","text":"tidyprompt added prompt_wrap() add check user accept decline result prompt, providing feedback result declined","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/user_verify.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Have user check the result of a prompt (human-in-the-loop) — user_verify","text":"","code":"if (FALSE) { # \\dontrun{   \"Tell me a fun fact about yourself!\" |>     user_verify() |>     send_prompt()   # --- Sending request to LLM provider (gpt-4o-mini): ---   # Tell me a fun fact about yourself!   # --- Receiving response from LLM provider: ---   # I don't have personal experiences or feelings, but a fun fact about me is that   # I can generate text in multiple languages! From English to Spanish, French, and   # more, I'm here to help with diverse linguistic needs.   #   # ── Evaluation of tidyprompt resulted in:   # [1] \"I don't have personal experiences or feelings, but a fun fact about me is   # that I can generate text in multiple languages! From English to Spanish, French,   # and more, I'm here to help with diverse linguistic needs.\"   #   # ── Accept or decline   # ℹ If satisfied, type nothing   # ℹ If not satisfied, type feedback to the LLM   # Type: Needs to be funnier!   # --- Sending request to LLM provider (gpt-4o-mini): ---   # Needs to be funnier!   # --- Receiving response from LLM provider: ---   # Alright, how about this: I once tried to tell a joke, but my punchline got lost   # in translation! Now, I just stick to delivering “byte-sized” humor!   #   # ── Evaluation of tidyprompt resulted in:   # [1] \"Alright, how about this: I once tried to tell a joke, but my punchline got   # lost in translation! Now, I just stick to delivering “byte-sized” humor!\"   #   # ── Accept or decline   # ℹ If satisfied, type nothing   # ℹ If not satisfied, type feedback to the LLM   # Type:   # ✔ Result accepted   # [1] \"Alright, how about this: I once tried to tell a joke, but my punchline got   # lost in translation! Now, I just stick to delivering “byte-sized” humor!\" } # }"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/vector_list_to_string.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a named or unnamed list/vector to a string representation — vector_list_to_string","title":"Convert a named or unnamed list/vector to a string representation — vector_list_to_string","text":"Converts named unnamed list/vector string format, intended sending LLM (display logging).","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/vector_list_to_string.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a named or unnamed list/vector to a string representation — vector_list_to_string","text":"","code":"vector_list_to_string(obj, how = c(\"inline\", \"expanded\"))"},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/vector_list_to_string.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a named or unnamed list/vector to a string representation — vector_list_to_string","text":"obj list vector (named unnamed) converted string. way object converted string; either \"inline\" \"expanded\". \"inline\" presents key-value pairs values single line. \"expanded\" presents key-value pair value separate line.","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/vector_list_to_string.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a named or unnamed list/vector to a string representation — vector_list_to_string","text":"single string representing list/vector.","code":""},{"path":[]},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/reference/vector_list_to_string.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a named or unnamed list/vector to a string representation — vector_list_to_string","text":"","code":"named_vector <- c(x = 10, y = 20, z = 30)  vector_list_to_string(named_vector, how = \"inline\") #> [1] \"x: 10, y: 20, z: 30\"  vector_list_to_string(named_vector, how = \"expanded\") #> [1] \"x: 10\\ny: 20\\nz: 30\""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/news/index.html","id":"tidyprompt-001","dir":"Changelog","previous_headings":"","what":"tidyprompt 0.0.1","title":"tidyprompt 0.0.1","text":"Initial CRAN submission","code":""},{"path":"https://tjarkvandemerwe.github.io/tidyprompt/news/index.html","id":"tidyprompt-0009000","dir":"Changelog","previous_headings":"","what":"tidyprompt 0.0.0.9000","title":"tidyprompt 0.0.0.9000","text":"Development version available GitHub","code":""}]
